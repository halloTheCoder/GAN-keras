{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation, Input, merge, multiply, Reshape, Multiply\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import optimizers, losses\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:\n",
    "    def __init__(self):\n",
    "        img_rows = 28\n",
    "        img_cols = 28\n",
    "        self.img_channel = 1\n",
    "        \n",
    "        self.img_shape = (img_rows, img_cols, self.img_channel)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "        \n",
    "        optimizer = optimizers.Adam(lr = 0.0002, beta_1 = 0.5, beta_2 = 0.999)\n",
    "        loss = [losses.binary_crossentropy, losses.sparse_categorical_crossentropy]\n",
    "        \n",
    "        # build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.summary()\n",
    "        self.discriminator.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])\n",
    "        \n",
    "        # build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.summary()\n",
    "        \n",
    "        # the generator takes input label and noise as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape = (self.latent_dim, ))\n",
    "        label = Input(shape = (1, ))\n",
    "        img = self.generator([noise, label])\n",
    "        \n",
    "        # for the combined model, we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # discriminator takes the img as input and determines its validity and label of that img\n",
    "        valid, target_label = self.discriminator(img)         # what is validity\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(inputs = [noise, label], outputs = [valid, target_label])\n",
    "        self.combined.compile(loss = loss, optimizer = optimizer)\n",
    "        \n",
    "        \n",
    "    def build_generator(self):\n",
    "        noise = Input(shape = (self.latent_dim, ))\n",
    "        label = Input(shape = (1, ), dtype = np.int32)\n",
    "        label_embedding = Embedding(input_dim = self.num_classes, output_dim = self.latent_dim, input_length = 1)(label)\n",
    "        label_embedding = Flatten()(label_embedding)\n",
    "        input_tensor = Multiply()([noise, label_embedding])\n",
    "#         input_tensor = multiply([noise, label_embedding])\n",
    "        \n",
    "        output = Dense(128 * 7 * 7, activation = 'relu')(input_tensor)\n",
    "        output = Reshape((7, 7, 128))(output)\n",
    "        output = BatchNormalization(momentum = 0.8)(output)\n",
    "        output = UpSampling2D(size = (2, 2))(output)\n",
    "        output = Conv2D(128, kernel_size = 3, padding = 'same', activation = 'relu')(output)\n",
    "        output = BatchNormalization(momentum = 0.8)(output)\n",
    "        output = UpSampling2D(size = (2, 2))(output)\n",
    "        output = Conv2D(64, kernel_size = 3, padding = 'same', activation = 'relu')(output)\n",
    "        output = BatchNormalization(momentum = 0.8)(output)\n",
    "        output = Conv2D(self.img_channel, kernel_size = 3, padding = 'same', activation = 'tanh')(output)\n",
    "        \n",
    "        return Model(inputs = [noise, label], outputs = output)\n",
    "        \n",
    "    def build_discriminator(self):\n",
    "        img = Input(shape = self.img_shape)\n",
    "        \n",
    "        output = Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\")(img)\n",
    "        output = LeakyReLU(alpha=0.2)(output)\n",
    "        output = Dropout(0.25)(output)\n",
    "        output = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(output)\n",
    "        output = ZeroPadding2D(padding=((0,1),(0,1)))(output)\n",
    "        output = LeakyReLU(alpha=0.2)(output)\n",
    "        output = Dropout(0.25)(output)\n",
    "        output = BatchNormalization(momentum=0.8)(output)\n",
    "        output = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(output)\n",
    "        output = LeakyReLU(alpha=0.2)(output)\n",
    "        output = Dropout(0.25)(output)\n",
    "        output = BatchNormalization(momentum=0.8)(output)\n",
    "        output = Conv2D(128, kernel_size=3, strides=1, padding=\"same\")(output)\n",
    "        output = LeakyReLU(alpha=0.2)(output)\n",
    "        output = Dropout(0.25)(output)\n",
    "        output = Flatten()(output)\n",
    "        \n",
    "        # determine validity and label of img\n",
    "        validity = Dense(1, activation = 'sigmoid')(output)\n",
    "        label = Dense(self.num_classes + 1, activation = 'softmax')(output)          #IMP :: (+1)\n",
    "        \n",
    "        return Model(img, [validity, label])\n",
    "    \n",
    "    def train(self, epochs, batch_size = 128, sample_interval = 50):\n",
    "        (X_train, y_train), (_, _) = mnist.load_data()\n",
    "        \n",
    "        # Scale the inputs\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis = -1)\n",
    "        y_train = y_train.reshape((-1, 1))\n",
    "        \n",
    "        # adverserial ground truths\n",
    "        fake = np.zeros(shape = (batch_size, ))\n",
    "        valid = np.ones(shape = (batch_size, ))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            idx = np.random.randint(0, len(X_train), batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            \n",
    "            noise = np.random.normal(size = (batch_size, self.latent_dim))\n",
    "            img_labels = y_train[idx]\n",
    "            fake_labels = 10 * np.ones(img_labels.shape)       #IMP :: ( * 10)\n",
    "            \n",
    "            # the labels of digits whose img representation is created by generator\n",
    "            sampled_labels = np.random.randint(0, 10, size = (batch_size, 1))\n",
    "            \n",
    "            # generated imgs\n",
    "            gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "            \n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, img_labels])\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, fake_labels])\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.save_model()\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1 (Linear Scaling)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator\")\n",
    "        save(self.discriminator, \"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 14, 14, 16)   160         input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, 14, 14, 16)   0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 14, 14, 16)   0           leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 7, 7, 32)     4640        dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 8, 8, 32)     0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, 8, 8, 32)     0           zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 8, 8, 32)     0           leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 8, 8, 32)     128         dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 4, 4, 64)     18496       batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 4, 4, 64)     0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 4, 4, 64)     0           leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 64)     256         dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 4, 4, 128)    73856       batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 4, 4, 128)    0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 4, 4, 128)    0           leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 2048)         0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1)            2049        flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 11)           22539       flatten_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 122,124\n",
      "Trainable params: 121,932\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_41 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 100)       1000        input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 100)          0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 100)          0           input_40[0][0]                   \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 6272)         633472      multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 7, 7, 128)    0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 7, 7, 128)    512         reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 14, 14, 128)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 14, 14, 128)  147584      up_sampling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 14, 14, 128)  512         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 28, 28, 128)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 28, 28, 64)   73792       up_sampling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 28, 28, 64)   256         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 28, 28, 1)    577         batch_normalization_47[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 857,705\n",
      "Trainable params: 857,065\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "acgan = ACGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 3.159256, acc.: 54.69%, op_acc: 7.81%] [G loss: 3.272259]\n",
      "1 [D loss: 3.324464, acc.: 45.31%, op_acc: 9.38%] [G loss: 3.235147]\n",
      "2 [D loss: 3.470210, acc.: 51.56%, op_acc: 10.94%] [G loss: 3.361943]\n",
      "3 [D loss: 3.037040, acc.: 56.25%, op_acc: 25.00%] [G loss: 3.531344]\n",
      "4 [D loss: 3.026678, acc.: 54.69%, op_acc: 23.44%] [G loss: 3.408584]\n",
      "5 [D loss: 2.915201, acc.: 50.00%, op_acc: 35.94%] [G loss: 3.645253]\n",
      "6 [D loss: 2.773649, acc.: 59.38%, op_acc: 45.31%] [G loss: 3.512623]\n",
      "7 [D loss: 2.638046, acc.: 54.69%, op_acc: 42.19%] [G loss: 3.766753]\n",
      "8 [D loss: 2.596419, acc.: 67.19%, op_acc: 43.75%] [G loss: 3.832509]\n",
      "9 [D loss: 2.542039, acc.: 60.94%, op_acc: 51.56%] [G loss: 4.037041]\n",
      "10 [D loss: 2.649679, acc.: 64.06%, op_acc: 45.31%] [G loss: 3.823858]\n",
      "11 [D loss: 2.522644, acc.: 65.62%, op_acc: 43.75%] [G loss: 3.972373]\n",
      "12 [D loss: 2.515530, acc.: 57.81%, op_acc: 46.88%] [G loss: 4.076571]\n",
      "13 [D loss: 2.456079, acc.: 59.38%, op_acc: 51.56%] [G loss: 4.097317]\n",
      "14 [D loss: 2.549984, acc.: 59.38%, op_acc: 53.12%] [G loss: 4.277263]\n",
      "15 [D loss: 2.353955, acc.: 59.38%, op_acc: 53.12%] [G loss: 4.324038]\n",
      "16 [D loss: 2.400322, acc.: 64.06%, op_acc: 48.44%] [G loss: 4.396139]\n",
      "17 [D loss: 2.238153, acc.: 65.62%, op_acc: 50.00%] [G loss: 4.556585]\n",
      "18 [D loss: 2.435894, acc.: 68.75%, op_acc: 46.88%] [G loss: 4.444315]\n",
      "19 [D loss: 2.413970, acc.: 57.81%, op_acc: 51.56%] [G loss: 4.626742]\n",
      "20 [D loss: 2.186276, acc.: 75.00%, op_acc: 50.00%] [G loss: 4.628422]\n",
      "21 [D loss: 2.194398, acc.: 82.81%, op_acc: 51.56%] [G loss: 4.751459]\n",
      "22 [D loss: 2.091512, acc.: 82.81%, op_acc: 53.12%] [G loss: 5.112634]\n",
      "23 [D loss: 2.195435, acc.: 75.00%, op_acc: 51.56%] [G loss: 5.062447]\n",
      "24 [D loss: 2.003866, acc.: 81.25%, op_acc: 57.81%] [G loss: 5.049048]\n",
      "25 [D loss: 1.924191, acc.: 73.44%, op_acc: 51.56%] [G loss: 5.168724]\n",
      "26 [D loss: 2.021373, acc.: 79.69%, op_acc: 53.12%] [G loss: 4.865486]\n",
      "27 [D loss: 1.943822, acc.: 87.50%, op_acc: 50.00%] [G loss: 4.962223]\n",
      "28 [D loss: 1.895779, acc.: 85.94%, op_acc: 56.25%] [G loss: 4.937058]\n",
      "29 [D loss: 1.755609, acc.: 85.94%, op_acc: 54.69%] [G loss: 5.086188]\n",
      "30 [D loss: 1.816664, acc.: 89.06%, op_acc: 48.44%] [G loss: 5.139942]\n",
      "31 [D loss: 1.878501, acc.: 81.25%, op_acc: 56.25%] [G loss: 5.064754]\n",
      "32 [D loss: 1.730851, acc.: 81.25%, op_acc: 56.25%] [G loss: 4.937307]\n",
      "33 [D loss: 1.789301, acc.: 84.38%, op_acc: 62.50%] [G loss: 5.242542]\n",
      "34 [D loss: 1.752435, acc.: 84.38%, op_acc: 53.12%] [G loss: 5.228928]\n",
      "35 [D loss: 1.738452, acc.: 78.12%, op_acc: 56.25%] [G loss: 4.974098]\n",
      "36 [D loss: 1.696414, acc.: 85.94%, op_acc: 65.62%] [G loss: 4.906997]\n",
      "37 [D loss: 1.953036, acc.: 78.12%, op_acc: 59.38%] [G loss: 4.757763]\n",
      "38 [D loss: 1.768715, acc.: 79.69%, op_acc: 57.81%] [G loss: 5.242597]\n",
      "39 [D loss: 1.631124, acc.: 85.94%, op_acc: 54.69%] [G loss: 5.753580]\n",
      "40 [D loss: 1.499125, acc.: 90.62%, op_acc: 73.44%] [G loss: 5.412597]\n",
      "41 [D loss: 1.738508, acc.: 85.94%, op_acc: 54.69%] [G loss: 5.426925]\n",
      "42 [D loss: 1.771701, acc.: 82.81%, op_acc: 57.81%] [G loss: 5.442302]\n",
      "43 [D loss: 1.717356, acc.: 78.12%, op_acc: 65.62%] [G loss: 4.649288]\n",
      "44 [D loss: 1.688081, acc.: 76.56%, op_acc: 68.75%] [G loss: 5.305482]\n",
      "45 [D loss: 1.906539, acc.: 73.44%, op_acc: 54.69%] [G loss: 5.140340]\n",
      "46 [D loss: 1.887025, acc.: 78.12%, op_acc: 54.69%] [G loss: 5.343799]\n",
      "47 [D loss: 1.858682, acc.: 64.06%, op_acc: 60.94%] [G loss: 4.621855]\n",
      "48 [D loss: 1.983187, acc.: 70.31%, op_acc: 54.69%] [G loss: 4.668501]\n",
      "49 [D loss: 1.686168, acc.: 78.12%, op_acc: 70.31%] [G loss: 4.753086]\n",
      "50 [D loss: 2.001968, acc.: 64.06%, op_acc: 57.81%] [G loss: 4.881117]\n",
      "51 [D loss: 1.909374, acc.: 68.75%, op_acc: 57.81%] [G loss: 4.670508]\n",
      "52 [D loss: 1.970382, acc.: 60.94%, op_acc: 59.38%] [G loss: 4.711927]\n",
      "53 [D loss: 1.809139, acc.: 68.75%, op_acc: 62.50%] [G loss: 5.007649]\n",
      "54 [D loss: 1.906968, acc.: 70.31%, op_acc: 60.94%] [G loss: 4.494042]\n",
      "55 [D loss: 1.803792, acc.: 64.06%, op_acc: 62.50%] [G loss: 5.126310]\n",
      "56 [D loss: 1.602456, acc.: 76.56%, op_acc: 67.19%] [G loss: 5.056271]\n",
      "57 [D loss: 1.806641, acc.: 73.44%, op_acc: 59.38%] [G loss: 5.254020]\n",
      "58 [D loss: 1.908158, acc.: 59.38%, op_acc: 60.94%] [G loss: 5.266688]\n",
      "59 [D loss: 1.673142, acc.: 87.50%, op_acc: 64.06%] [G loss: 5.297135]\n",
      "60 [D loss: 1.800429, acc.: 67.19%, op_acc: 59.38%] [G loss: 4.918185]\n",
      "61 [D loss: 2.128527, acc.: 57.81%, op_acc: 56.25%] [G loss: 5.021170]\n",
      "62 [D loss: 1.785040, acc.: 67.19%, op_acc: 67.19%] [G loss: 4.890468]\n",
      "63 [D loss: 1.741413, acc.: 64.06%, op_acc: 59.38%] [G loss: 5.211543]\n",
      "64 [D loss: 1.656648, acc.: 65.62%, op_acc: 68.75%] [G loss: 5.587741]\n",
      "65 [D loss: 1.752149, acc.: 68.75%, op_acc: 62.50%] [G loss: 5.261207]\n",
      "66 [D loss: 1.620698, acc.: 71.88%, op_acc: 68.75%] [G loss: 5.621761]\n",
      "67 [D loss: 1.535215, acc.: 85.94%, op_acc: 70.31%] [G loss: 5.696862]\n",
      "68 [D loss: 1.462814, acc.: 82.81%, op_acc: 70.31%] [G loss: 6.172650]\n",
      "69 [D loss: 1.486245, acc.: 75.00%, op_acc: 62.50%] [G loss: 6.090913]\n",
      "70 [D loss: 1.580335, acc.: 85.94%, op_acc: 60.94%] [G loss: 6.072719]\n",
      "71 [D loss: 1.570967, acc.: 85.94%, op_acc: 65.62%] [G loss: 6.445727]\n",
      "72 [D loss: 1.430805, acc.: 81.25%, op_acc: 73.44%] [G loss: 6.478159]\n",
      "73 [D loss: 1.286374, acc.: 92.19%, op_acc: 70.31%] [G loss: 6.377563]\n",
      "74 [D loss: 1.215803, acc.: 95.31%, op_acc: 70.31%] [G loss: 6.263803]\n",
      "75 [D loss: 1.159436, acc.: 95.31%, op_acc: 71.88%] [G loss: 7.256658]\n",
      "76 [D loss: 1.139508, acc.: 90.62%, op_acc: 78.12%] [G loss: 6.761939]\n",
      "77 [D loss: 1.355183, acc.: 89.06%, op_acc: 68.75%] [G loss: 6.253300]\n",
      "78 [D loss: 1.084966, acc.: 87.50%, op_acc: 76.56%] [G loss: 6.384876]\n",
      "79 [D loss: 1.375491, acc.: 84.38%, op_acc: 76.56%] [G loss: 6.330340]\n",
      "80 [D loss: 1.277477, acc.: 89.06%, op_acc: 70.31%] [G loss: 7.484373]\n",
      "81 [D loss: 1.033099, acc.: 95.31%, op_acc: 75.00%] [G loss: 7.626506]\n",
      "82 [D loss: 1.020838, acc.: 96.88%, op_acc: 81.25%] [G loss: 7.361845]\n",
      "83 [D loss: 1.195630, acc.: 93.75%, op_acc: 73.44%] [G loss: 7.021555]\n",
      "84 [D loss: 1.160033, acc.: 96.88%, op_acc: 73.44%] [G loss: 7.336302]\n",
      "85 [D loss: 0.973515, acc.: 96.88%, op_acc: 76.56%] [G loss: 7.455820]\n",
      "86 [D loss: 0.844274, acc.: 95.31%, op_acc: 79.69%] [G loss: 7.255284]\n",
      "87 [D loss: 0.898614, acc.: 100.00%, op_acc: 78.12%] [G loss: 8.075703]\n",
      "88 [D loss: 0.972292, acc.: 98.44%, op_acc: 76.56%] [G loss: 7.713899]\n",
      "89 [D loss: 0.766953, acc.: 96.88%, op_acc: 84.38%] [G loss: 8.510803]\n",
      "90 [D loss: 0.978047, acc.: 98.44%, op_acc: 73.44%] [G loss: 8.261227]\n",
      "91 [D loss: 0.884964, acc.: 100.00%, op_acc: 79.69%] [G loss: 7.940143]\n",
      "92 [D loss: 0.824531, acc.: 98.44%, op_acc: 75.00%] [G loss: 8.527651]\n",
      "93 [D loss: 0.972180, acc.: 96.88%, op_acc: 68.75%] [G loss: 8.659191]\n",
      "94 [D loss: 0.755914, acc.: 100.00%, op_acc: 79.69%] [G loss: 9.200119]\n",
      "95 [D loss: 0.856758, acc.: 98.44%, op_acc: 76.56%] [G loss: 8.511127]\n",
      "96 [D loss: 0.826671, acc.: 96.88%, op_acc: 81.25%] [G loss: 8.501730]\n",
      "97 [D loss: 0.840669, acc.: 98.44%, op_acc: 82.81%] [G loss: 7.943560]\n",
      "98 [D loss: 0.802062, acc.: 98.44%, op_acc: 70.31%] [G loss: 8.014685]\n",
      "99 [D loss: 0.803268, acc.: 96.88%, op_acc: 78.12%] [G loss: 8.219443]\n",
      "100 [D loss: 0.833251, acc.: 100.00%, op_acc: 76.56%] [G loss: 7.628450]\n",
      "101 [D loss: 0.803565, acc.: 100.00%, op_acc: 82.81%] [G loss: 8.156486]\n",
      "102 [D loss: 0.717770, acc.: 98.44%, op_acc: 76.56%] [G loss: 8.436898]\n",
      "103 [D loss: 0.949201, acc.: 98.44%, op_acc: 73.44%] [G loss: 8.906250]\n",
      "104 [D loss: 0.672880, acc.: 98.44%, op_acc: 87.50%] [G loss: 9.239431]\n",
      "105 [D loss: 0.885634, acc.: 98.44%, op_acc: 76.56%] [G loss: 9.377673]\n",
      "106 [D loss: 0.667221, acc.: 100.00%, op_acc: 90.62%] [G loss: 9.413351]\n",
      "107 [D loss: 0.614480, acc.: 98.44%, op_acc: 90.62%] [G loss: 10.185741]\n",
      "108 [D loss: 0.787952, acc.: 100.00%, op_acc: 84.38%] [G loss: 9.750053]\n",
      "109 [D loss: 0.655223, acc.: 100.00%, op_acc: 82.81%] [G loss: 9.550127]\n",
      "110 [D loss: 0.708037, acc.: 100.00%, op_acc: 78.12%] [G loss: 10.675934]\n",
      "111 [D loss: 0.638702, acc.: 98.44%, op_acc: 81.25%] [G loss: 9.804340]\n",
      "112 [D loss: 0.440336, acc.: 100.00%, op_acc: 92.19%] [G loss: 10.634987]\n",
      "113 [D loss: 0.626803, acc.: 100.00%, op_acc: 82.81%] [G loss: 9.924408]\n",
      "114 [D loss: 0.569974, acc.: 100.00%, op_acc: 84.38%] [G loss: 10.455612]\n",
      "115 [D loss: 0.681621, acc.: 100.00%, op_acc: 79.69%] [G loss: 10.179358]\n",
      "116 [D loss: 0.601407, acc.: 100.00%, op_acc: 85.94%] [G loss: 9.663801]\n",
      "117 [D loss: 0.639712, acc.: 100.00%, op_acc: 75.00%] [G loss: 10.517035]\n",
      "118 [D loss: 0.570557, acc.: 100.00%, op_acc: 78.12%] [G loss: 10.055947]\n",
      "119 [D loss: 0.647937, acc.: 100.00%, op_acc: 81.25%] [G loss: 9.183569]\n",
      "120 [D loss: 0.693445, acc.: 96.88%, op_acc: 84.38%] [G loss: 9.459197]\n",
      "121 [D loss: 0.493436, acc.: 100.00%, op_acc: 85.94%] [G loss: 8.398724]\n",
      "122 [D loss: 0.976900, acc.: 98.44%, op_acc: 73.44%] [G loss: 7.958261]\n",
      "123 [D loss: 1.241459, acc.: 85.94%, op_acc: 73.44%] [G loss: 7.914053]\n",
      "124 [D loss: 1.614627, acc.: 65.62%, op_acc: 65.62%] [G loss: 6.602896]\n",
      "125 [D loss: 1.105347, acc.: 81.25%, op_acc: 81.25%] [G loss: 8.166470]\n",
      "126 [D loss: 0.880992, acc.: 87.50%, op_acc: 87.50%] [G loss: 8.464432]\n",
      "127 [D loss: 0.974553, acc.: 93.75%, op_acc: 78.12%] [G loss: 8.283142]\n",
      "128 [D loss: 0.980351, acc.: 82.81%, op_acc: 85.94%] [G loss: 6.275510]\n",
      "129 [D loss: 1.291604, acc.: 75.00%, op_acc: 82.81%] [G loss: 7.285422]\n",
      "130 [D loss: 1.290898, acc.: 73.44%, op_acc: 73.44%] [G loss: 7.733673]\n",
      "131 [D loss: 0.956365, acc.: 96.88%, op_acc: 82.81%] [G loss: 8.510016]\n",
      "132 [D loss: 1.104920, acc.: 96.88%, op_acc: 70.31%] [G loss: 8.316252]\n",
      "133 [D loss: 0.778183, acc.: 95.31%, op_acc: 85.94%] [G loss: 8.371710]\n",
      "134 [D loss: 0.897910, acc.: 84.38%, op_acc: 84.38%] [G loss: 7.833575]\n",
      "135 [D loss: 0.964970, acc.: 93.75%, op_acc: 81.25%] [G loss: 7.411607]\n",
      "136 [D loss: 0.873227, acc.: 95.31%, op_acc: 78.12%] [G loss: 8.175299]\n",
      "137 [D loss: 0.591509, acc.: 98.44%, op_acc: 90.62%] [G loss: 7.616994]\n",
      "138 [D loss: 0.876569, acc.: 100.00%, op_acc: 79.69%] [G loss: 7.746587]\n",
      "139 [D loss: 0.893262, acc.: 92.19%, op_acc: 82.81%] [G loss: 6.999873]\n",
      "140 [D loss: 1.007418, acc.: 93.75%, op_acc: 81.25%] [G loss: 6.991446]\n",
      "141 [D loss: 0.943475, acc.: 92.19%, op_acc: 87.50%] [G loss: 6.915749]\n",
      "142 [D loss: 0.961231, acc.: 89.06%, op_acc: 82.81%] [G loss: 7.153419]\n",
      "143 [D loss: 1.004048, acc.: 87.50%, op_acc: 87.50%] [G loss: 6.711735]\n",
      "144 [D loss: 0.819996, acc.: 89.06%, op_acc: 85.94%] [G loss: 5.779909]\n",
      "145 [D loss: 1.405139, acc.: 65.62%, op_acc: 78.12%] [G loss: 6.391334]\n",
      "146 [D loss: 1.134101, acc.: 84.38%, op_acc: 81.25%] [G loss: 8.094378]\n",
      "147 [D loss: 1.036243, acc.: 85.94%, op_acc: 79.69%] [G loss: 7.982072]\n",
      "148 [D loss: 0.899249, acc.: 90.62%, op_acc: 81.25%] [G loss: 7.775084]\n",
      "149 [D loss: 0.954228, acc.: 90.62%, op_acc: 82.81%] [G loss: 7.234533]\n",
      "150 [D loss: 1.184157, acc.: 92.19%, op_acc: 71.88%] [G loss: 7.042891]\n",
      "151 [D loss: 0.904423, acc.: 93.75%, op_acc: 89.06%] [G loss: 8.476390]\n",
      "152 [D loss: 1.012527, acc.: 92.19%, op_acc: 76.56%] [G loss: 8.053303]\n",
      "153 [D loss: 1.133255, acc.: 93.75%, op_acc: 75.00%] [G loss: 7.908770]\n",
      "154 [D loss: 1.089386, acc.: 87.50%, op_acc: 78.12%] [G loss: 8.060178]\n",
      "155 [D loss: 1.067486, acc.: 89.06%, op_acc: 79.69%] [G loss: 7.967956]\n",
      "156 [D loss: 1.082185, acc.: 90.62%, op_acc: 79.69%] [G loss: 7.711734]\n",
      "157 [D loss: 0.760261, acc.: 95.31%, op_acc: 82.81%] [G loss: 7.520731]\n",
      "158 [D loss: 0.995382, acc.: 89.06%, op_acc: 85.94%] [G loss: 7.887981]\n",
      "159 [D loss: 1.360627, acc.: 70.31%, op_acc: 79.69%] [G loss: 7.396641]\n",
      "160 [D loss: 1.204442, acc.: 84.38%, op_acc: 79.69%] [G loss: 7.280442]\n",
      "161 [D loss: 1.095317, acc.: 85.94%, op_acc: 82.81%] [G loss: 6.577314]\n",
      "162 [D loss: 1.760632, acc.: 59.38%, op_acc: 70.31%] [G loss: 5.118406]\n",
      "163 [D loss: 1.349967, acc.: 76.56%, op_acc: 78.12%] [G loss: 5.309645]\n",
      "164 [D loss: 2.288899, acc.: 40.62%, op_acc: 62.50%] [G loss: 5.070624]\n",
      "165 [D loss: 2.406417, acc.: 32.81%, op_acc: 54.69%] [G loss: 5.570879]\n",
      "166 [D loss: 1.849718, acc.: 53.12%, op_acc: 70.31%] [G loss: 6.143970]\n",
      "167 [D loss: 2.194131, acc.: 43.75%, op_acc: 57.81%] [G loss: 5.903280]\n",
      "168 [D loss: 2.273986, acc.: 32.81%, op_acc: 65.62%] [G loss: 5.421022]\n",
      "169 [D loss: 2.301486, acc.: 35.94%, op_acc: 60.94%] [G loss: 5.678803]\n",
      "170 [D loss: 2.632762, acc.: 23.44%, op_acc: 56.25%] [G loss: 4.893733]\n",
      "171 [D loss: 2.769918, acc.: 15.62%, op_acc: 48.44%] [G loss: 5.281904]\n",
      "172 [D loss: 2.508705, acc.: 26.56%, op_acc: 59.38%] [G loss: 5.112540]\n",
      "173 [D loss: 2.282556, acc.: 31.25%, op_acc: 57.81%] [G loss: 4.793706]\n",
      "174 [D loss: 2.499841, acc.: 39.06%, op_acc: 51.56%] [G loss: 5.586428]\n",
      "175 [D loss: 2.035002, acc.: 40.62%, op_acc: 67.19%] [G loss: 5.961634]\n",
      "176 [D loss: 1.982193, acc.: 48.44%, op_acc: 59.38%] [G loss: 6.004369]\n",
      "177 [D loss: 1.926773, acc.: 37.50%, op_acc: 70.31%] [G loss: 6.447793]\n",
      "178 [D loss: 2.052457, acc.: 43.75%, op_acc: 67.19%] [G loss: 6.267661]\n",
      "179 [D loss: 2.339629, acc.: 45.31%, op_acc: 60.94%] [G loss: 5.701181]\n",
      "180 [D loss: 1.974136, acc.: 43.75%, op_acc: 67.19%] [G loss: 5.697710]\n",
      "181 [D loss: 2.040176, acc.: 37.50%, op_acc: 68.75%] [G loss: 6.226581]\n",
      "182 [D loss: 1.719986, acc.: 51.56%, op_acc: 67.19%] [G loss: 6.762694]\n",
      "183 [D loss: 1.760239, acc.: 46.88%, op_acc: 68.75%] [G loss: 5.987108]\n",
      "184 [D loss: 1.668543, acc.: 53.12%, op_acc: 71.88%] [G loss: 6.179802]\n",
      "185 [D loss: 2.387649, acc.: 42.19%, op_acc: 59.38%] [G loss: 5.926296]\n",
      "186 [D loss: 2.132995, acc.: 39.06%, op_acc: 65.62%] [G loss: 5.656444]\n",
      "187 [D loss: 1.903348, acc.: 46.88%, op_acc: 68.75%] [G loss: 6.215089]\n",
      "188 [D loss: 1.738330, acc.: 56.25%, op_acc: 73.44%] [G loss: 6.304501]\n",
      "189 [D loss: 1.805385, acc.: 39.06%, op_acc: 64.06%] [G loss: 5.771499]\n",
      "190 [D loss: 1.646682, acc.: 39.06%, op_acc: 75.00%] [G loss: 6.203362]\n",
      "191 [D loss: 1.751486, acc.: 48.44%, op_acc: 73.44%] [G loss: 5.954175]\n",
      "192 [D loss: 1.971607, acc.: 42.19%, op_acc: 59.38%] [G loss: 5.534706]\n",
      "193 [D loss: 1.843889, acc.: 46.88%, op_acc: 71.88%] [G loss: 6.326780]\n",
      "194 [D loss: 1.961159, acc.: 29.69%, op_acc: 68.75%] [G loss: 6.230113]\n",
      "195 [D loss: 1.650370, acc.: 51.56%, op_acc: 75.00%] [G loss: 5.858815]\n",
      "196 [D loss: 1.593764, acc.: 53.12%, op_acc: 76.56%] [G loss: 5.811162]\n",
      "197 [D loss: 1.729729, acc.: 51.56%, op_acc: 73.44%] [G loss: 6.284616]\n",
      "198 [D loss: 1.833915, acc.: 51.56%, op_acc: 67.19%] [G loss: 6.322515]\n",
      "199 [D loss: 1.569717, acc.: 50.00%, op_acc: 76.56%] [G loss: 5.862876]\n",
      "200 [D loss: 1.733492, acc.: 53.12%, op_acc: 68.75%] [G loss: 6.027283]\n",
      "201 [D loss: 1.506658, acc.: 54.69%, op_acc: 68.75%] [G loss: 5.597711]\n",
      "202 [D loss: 1.745699, acc.: 50.00%, op_acc: 73.44%] [G loss: 6.852307]\n",
      "203 [D loss: 2.014243, acc.: 34.38%, op_acc: 65.62%] [G loss: 6.610042]\n",
      "204 [D loss: 1.590551, acc.: 56.25%, op_acc: 73.44%] [G loss: 6.378847]\n",
      "205 [D loss: 1.895019, acc.: 48.44%, op_acc: 62.50%] [G loss: 7.270821]\n",
      "206 [D loss: 1.858422, acc.: 51.56%, op_acc: 60.94%] [G loss: 7.921724]\n",
      "207 [D loss: 1.446836, acc.: 59.38%, op_acc: 79.69%] [G loss: 7.476656]\n",
      "208 [D loss: 1.672750, acc.: 43.75%, op_acc: 76.56%] [G loss: 6.821523]\n",
      "209 [D loss: 2.934108, acc.: 31.25%, op_acc: 50.00%] [G loss: 6.408001]\n",
      "210 [D loss: 1.853066, acc.: 42.19%, op_acc: 73.44%] [G loss: 7.481225]\n",
      "211 [D loss: 1.461391, acc.: 54.69%, op_acc: 78.12%] [G loss: 6.230460]\n",
      "212 [D loss: 2.047330, acc.: 50.00%, op_acc: 59.38%] [G loss: 6.182008]\n",
      "213 [D loss: 1.688850, acc.: 51.56%, op_acc: 70.31%] [G loss: 7.288086]\n",
      "214 [D loss: 2.038280, acc.: 31.25%, op_acc: 64.06%] [G loss: 6.633351]\n",
      "215 [D loss: 1.799012, acc.: 42.19%, op_acc: 67.19%] [G loss: 6.070178]\n",
      "216 [D loss: 2.121333, acc.: 51.56%, op_acc: 51.56%] [G loss: 6.121464]\n",
      "217 [D loss: 1.620911, acc.: 48.44%, op_acc: 73.44%] [G loss: 6.528007]\n",
      "218 [D loss: 1.462567, acc.: 62.50%, op_acc: 76.56%] [G loss: 7.021194]\n",
      "219 [D loss: 1.575662, acc.: 54.69%, op_acc: 75.00%] [G loss: 5.840817]\n",
      "220 [D loss: 2.047314, acc.: 35.94%, op_acc: 67.19%] [G loss: 5.865777]\n",
      "221 [D loss: 2.233581, acc.: 31.25%, op_acc: 71.88%] [G loss: 5.404245]\n",
      "222 [D loss: 2.050031, acc.: 31.25%, op_acc: 73.44%] [G loss: 5.527754]\n",
      "223 [D loss: 1.721255, acc.: 39.06%, op_acc: 71.88%] [G loss: 6.235326]\n",
      "224 [D loss: 2.276938, acc.: 31.25%, op_acc: 60.94%] [G loss: 6.657843]\n",
      "225 [D loss: 1.820300, acc.: 50.00%, op_acc: 70.31%] [G loss: 6.415118]\n",
      "226 [D loss: 1.793525, acc.: 42.19%, op_acc: 60.94%] [G loss: 6.178810]\n",
      "227 [D loss: 1.443026, acc.: 59.38%, op_acc: 70.31%] [G loss: 6.266933]\n",
      "228 [D loss: 1.247466, acc.: 65.62%, op_acc: 81.25%] [G loss: 6.867909]\n",
      "229 [D loss: 1.416382, acc.: 71.88%, op_acc: 79.69%] [G loss: 6.878281]\n",
      "230 [D loss: 1.602453, acc.: 57.81%, op_acc: 73.44%] [G loss: 7.513220]\n",
      "231 [D loss: 1.388346, acc.: 68.75%, op_acc: 71.88%] [G loss: 7.112263]\n",
      "232 [D loss: 1.311328, acc.: 76.56%, op_acc: 73.44%] [G loss: 6.915064]\n",
      "233 [D loss: 1.192852, acc.: 85.94%, op_acc: 81.25%] [G loss: 7.148642]\n",
      "234 [D loss: 1.294870, acc.: 71.88%, op_acc: 82.81%] [G loss: 7.475632]\n",
      "235 [D loss: 1.268426, acc.: 73.44%, op_acc: 70.31%] [G loss: 8.125458]\n",
      "236 [D loss: 1.487617, acc.: 64.06%, op_acc: 81.25%] [G loss: 7.825809]\n",
      "237 [D loss: 1.465525, acc.: 65.62%, op_acc: 75.00%] [G loss: 7.256955]\n",
      "238 [D loss: 1.507500, acc.: 62.50%, op_acc: 75.00%] [G loss: 7.127795]\n",
      "239 [D loss: 1.455582, acc.: 65.62%, op_acc: 68.75%] [G loss: 6.559799]\n",
      "240 [D loss: 1.436937, acc.: 59.38%, op_acc: 76.56%] [G loss: 5.453609]\n",
      "241 [D loss: 1.959881, acc.: 40.62%, op_acc: 67.19%] [G loss: 5.876584]\n",
      "242 [D loss: 2.176395, acc.: 39.06%, op_acc: 60.94%] [G loss: 5.904998]\n",
      "243 [D loss: 2.914109, acc.: 26.56%, op_acc: 31.25%] [G loss: 5.843442]\n",
      "244 [D loss: 2.458166, acc.: 29.69%, op_acc: 59.38%] [G loss: 6.528186]\n",
      "245 [D loss: 1.938498, acc.: 40.62%, op_acc: 68.75%] [G loss: 7.282564]\n",
      "246 [D loss: 1.762068, acc.: 50.00%, op_acc: 78.12%] [G loss: 6.039644]\n",
      "247 [D loss: 1.577767, acc.: 57.81%, op_acc: 75.00%] [G loss: 7.219529]\n",
      "248 [D loss: 1.869680, acc.: 54.69%, op_acc: 73.44%] [G loss: 7.394095]\n",
      "249 [D loss: 1.543185, acc.: 62.50%, op_acc: 67.19%] [G loss: 6.593626]\n",
      "250 [D loss: 1.438986, acc.: 57.81%, op_acc: 70.31%] [G loss: 6.661778]\n",
      "251 [D loss: 1.613308, acc.: 59.38%, op_acc: 67.19%] [G loss: 6.238292]\n",
      "252 [D loss: 1.631268, acc.: 46.88%, op_acc: 70.31%] [G loss: 6.546591]\n",
      "253 [D loss: 1.119329, acc.: 76.56%, op_acc: 82.81%] [G loss: 6.808067]\n",
      "254 [D loss: 1.290715, acc.: 78.12%, op_acc: 71.88%] [G loss: 6.860933]\n",
      "255 [D loss: 1.065353, acc.: 76.56%, op_acc: 85.94%] [G loss: 6.970788]\n",
      "256 [D loss: 1.332334, acc.: 76.56%, op_acc: 79.69%] [G loss: 7.073930]\n",
      "257 [D loss: 1.168389, acc.: 79.69%, op_acc: 73.44%] [G loss: 7.392901]\n",
      "258 [D loss: 1.012598, acc.: 84.38%, op_acc: 82.81%] [G loss: 7.133735]\n",
      "259 [D loss: 1.100925, acc.: 71.88%, op_acc: 79.69%] [G loss: 7.664664]\n",
      "260 [D loss: 1.163730, acc.: 73.44%, op_acc: 81.25%] [G loss: 6.002086]\n",
      "261 [D loss: 1.380425, acc.: 70.31%, op_acc: 76.56%] [G loss: 7.122626]\n",
      "262 [D loss: 1.276857, acc.: 79.69%, op_acc: 73.44%] [G loss: 6.653456]\n",
      "263 [D loss: 1.154060, acc.: 75.00%, op_acc: 82.81%] [G loss: 6.969884]\n",
      "264 [D loss: 1.379146, acc.: 68.75%, op_acc: 82.81%] [G loss: 6.054155]\n",
      "265 [D loss: 1.612608, acc.: 54.69%, op_acc: 73.44%] [G loss: 6.548584]\n",
      "266 [D loss: 1.702137, acc.: 51.56%, op_acc: 73.44%] [G loss: 5.828980]\n",
      "267 [D loss: 1.663027, acc.: 51.56%, op_acc: 73.44%] [G loss: 5.869117]\n",
      "268 [D loss: 1.675619, acc.: 51.56%, op_acc: 68.75%] [G loss: 6.636065]\n",
      "269 [D loss: 1.574146, acc.: 51.56%, op_acc: 79.69%] [G loss: 6.808889]\n",
      "270 [D loss: 1.662998, acc.: 46.88%, op_acc: 68.75%] [G loss: 6.056651]\n",
      "271 [D loss: 1.675386, acc.: 56.25%, op_acc: 75.00%] [G loss: 6.611884]\n",
      "272 [D loss: 1.471007, acc.: 59.38%, op_acc: 73.44%] [G loss: 6.584251]\n",
      "273 [D loss: 1.643671, acc.: 71.88%, op_acc: 68.75%] [G loss: 6.665844]\n",
      "274 [D loss: 1.324379, acc.: 73.44%, op_acc: 78.12%] [G loss: 6.659479]\n",
      "275 [D loss: 1.305679, acc.: 75.00%, op_acc: 82.81%] [G loss: 6.706731]\n",
      "276 [D loss: 1.644549, acc.: 43.75%, op_acc: 71.88%] [G loss: 6.229097]\n",
      "277 [D loss: 1.880893, acc.: 43.75%, op_acc: 59.38%] [G loss: 6.488796]\n",
      "278 [D loss: 1.238072, acc.: 67.19%, op_acc: 84.38%] [G loss: 6.608659]\n",
      "279 [D loss: 1.488114, acc.: 75.00%, op_acc: 70.31%] [G loss: 7.053481]\n",
      "280 [D loss: 1.623655, acc.: 68.75%, op_acc: 64.06%] [G loss: 6.579301]\n",
      "281 [D loss: 1.509747, acc.: 62.50%, op_acc: 71.88%] [G loss: 7.116869]\n",
      "282 [D loss: 1.296654, acc.: 64.06%, op_acc: 81.25%] [G loss: 5.866861]\n",
      "283 [D loss: 1.502642, acc.: 48.44%, op_acc: 76.56%] [G loss: 6.049651]\n",
      "284 [D loss: 1.248801, acc.: 60.94%, op_acc: 79.69%] [G loss: 5.788030]\n",
      "285 [D loss: 1.169444, acc.: 73.44%, op_acc: 78.12%] [G loss: 6.482467]\n",
      "286 [D loss: 1.495083, acc.: 62.50%, op_acc: 76.56%] [G loss: 6.544760]\n",
      "287 [D loss: 1.414012, acc.: 53.12%, op_acc: 79.69%] [G loss: 6.591211]\n",
      "288 [D loss: 1.310621, acc.: 56.25%, op_acc: 81.25%] [G loss: 7.194278]\n",
      "289 [D loss: 1.577894, acc.: 59.38%, op_acc: 64.06%] [G loss: 7.136158]\n",
      "290 [D loss: 1.337746, acc.: 64.06%, op_acc: 78.12%] [G loss: 6.676192]\n",
      "291 [D loss: 1.165812, acc.: 75.00%, op_acc: 82.81%] [G loss: 6.477953]\n",
      "292 [D loss: 1.214786, acc.: 71.88%, op_acc: 78.12%] [G loss: 6.260540]\n",
      "293 [D loss: 1.391311, acc.: 62.50%, op_acc: 76.56%] [G loss: 6.542322]\n",
      "294 [D loss: 1.483530, acc.: 59.38%, op_acc: 75.00%] [G loss: 6.332516]\n",
      "295 [D loss: 1.382523, acc.: 67.19%, op_acc: 75.00%] [G loss: 6.274900]\n",
      "296 [D loss: 1.768363, acc.: 53.12%, op_acc: 78.12%] [G loss: 6.310561]\n",
      "297 [D loss: 1.381873, acc.: 56.25%, op_acc: 75.00%] [G loss: 5.547612]\n",
      "298 [D loss: 1.341674, acc.: 65.62%, op_acc: 75.00%] [G loss: 6.565928]\n",
      "299 [D loss: 1.204278, acc.: 71.88%, op_acc: 76.56%] [G loss: 6.513031]\n",
      "300 [D loss: 1.379741, acc.: 60.94%, op_acc: 75.00%] [G loss: 6.945926]\n",
      "301 [D loss: 1.248162, acc.: 73.44%, op_acc: 71.88%] [G loss: 6.622173]\n",
      "302 [D loss: 1.598701, acc.: 67.19%, op_acc: 79.69%] [G loss: 6.610785]\n",
      "303 [D loss: 1.707790, acc.: 73.44%, op_acc: 71.88%] [G loss: 7.080932]\n",
      "304 [D loss: 1.040240, acc.: 68.75%, op_acc: 81.25%] [G loss: 7.449014]\n",
      "305 [D loss: 1.137559, acc.: 68.75%, op_acc: 87.50%] [G loss: 7.685828]\n",
      "306 [D loss: 1.061318, acc.: 73.44%, op_acc: 84.38%] [G loss: 7.485050]\n",
      "307 [D loss: 1.294139, acc.: 65.62%, op_acc: 79.69%] [G loss: 6.657313]\n",
      "308 [D loss: 1.272566, acc.: 70.31%, op_acc: 79.69%] [G loss: 6.697460]\n",
      "309 [D loss: 1.089660, acc.: 75.00%, op_acc: 81.25%] [G loss: 6.784472]\n",
      "310 [D loss: 1.439114, acc.: 64.06%, op_acc: 79.69%] [G loss: 6.404658]\n",
      "311 [D loss: 1.575775, acc.: 67.19%, op_acc: 79.69%] [G loss: 6.220248]\n",
      "312 [D loss: 1.375831, acc.: 59.38%, op_acc: 78.12%] [G loss: 7.086913]\n",
      "313 [D loss: 1.181079, acc.: 62.50%, op_acc: 79.69%] [G loss: 6.090440]\n",
      "314 [D loss: 1.023170, acc.: 76.56%, op_acc: 81.25%] [G loss: 6.338366]\n",
      "315 [D loss: 1.292141, acc.: 75.00%, op_acc: 78.12%] [G loss: 6.638449]\n",
      "316 [D loss: 1.157545, acc.: 70.31%, op_acc: 79.69%] [G loss: 7.297281]\n",
      "317 [D loss: 1.325701, acc.: 73.44%, op_acc: 75.00%] [G loss: 7.418726]\n",
      "318 [D loss: 1.393512, acc.: 64.06%, op_acc: 73.44%] [G loss: 7.543597]\n",
      "319 [D loss: 1.376896, acc.: 54.69%, op_acc: 79.69%] [G loss: 6.644327]\n",
      "320 [D loss: 1.638923, acc.: 57.81%, op_acc: 78.12%] [G loss: 6.128994]\n",
      "321 [D loss: 1.398081, acc.: 54.69%, op_acc: 76.56%] [G loss: 6.553043]\n",
      "322 [D loss: 1.158348, acc.: 78.12%, op_acc: 81.25%] [G loss: 6.630000]\n",
      "323 [D loss: 1.069869, acc.: 68.75%, op_acc: 84.38%] [G loss: 6.572940]\n",
      "324 [D loss: 1.197152, acc.: 79.69%, op_acc: 75.00%] [G loss: 7.462308]\n",
      "325 [D loss: 1.163478, acc.: 79.69%, op_acc: 84.38%] [G loss: 6.941340]\n",
      "326 [D loss: 1.208675, acc.: 65.62%, op_acc: 81.25%] [G loss: 6.926126]\n",
      "327 [D loss: 1.059129, acc.: 78.12%, op_acc: 84.38%] [G loss: 6.636049]\n",
      "328 [D loss: 0.949635, acc.: 79.69%, op_acc: 82.81%] [G loss: 6.586365]\n",
      "329 [D loss: 1.665189, acc.: 50.00%, op_acc: 68.75%] [G loss: 5.464543]\n",
      "330 [D loss: 2.105346, acc.: 48.44%, op_acc: 65.62%] [G loss: 5.805525]\n",
      "331 [D loss: 1.549680, acc.: 56.25%, op_acc: 78.12%] [G loss: 5.894353]\n",
      "332 [D loss: 1.763285, acc.: 57.81%, op_acc: 68.75%] [G loss: 5.780036]\n",
      "333 [D loss: 2.007700, acc.: 26.56%, op_acc: 68.75%] [G loss: 5.232815]\n",
      "334 [D loss: 1.577943, acc.: 53.12%, op_acc: 78.12%] [G loss: 5.951766]\n",
      "335 [D loss: 1.797165, acc.: 50.00%, op_acc: 70.31%] [G loss: 6.327707]\n",
      "336 [D loss: 1.814952, acc.: 57.81%, op_acc: 67.19%] [G loss: 6.304366]\n",
      "337 [D loss: 1.841023, acc.: 53.12%, op_acc: 71.88%] [G loss: 7.087634]\n",
      "338 [D loss: 1.377880, acc.: 46.88%, op_acc: 81.25%] [G loss: 6.773047]\n",
      "339 [D loss: 1.956849, acc.: 62.50%, op_acc: 68.75%] [G loss: 7.198256]\n",
      "340 [D loss: 1.504319, acc.: 62.50%, op_acc: 79.69%] [G loss: 7.325062]\n",
      "341 [D loss: 1.440187, acc.: 53.12%, op_acc: 78.12%] [G loss: 6.223519]\n",
      "342 [D loss: 1.851834, acc.: 46.88%, op_acc: 62.50%] [G loss: 6.955929]\n",
      "343 [D loss: 1.867470, acc.: 54.69%, op_acc: 60.94%] [G loss: 6.364604]\n",
      "344 [D loss: 1.473494, acc.: 59.38%, op_acc: 70.31%] [G loss: 7.250929]\n",
      "345 [D loss: 1.280113, acc.: 65.62%, op_acc: 73.44%] [G loss: 6.767869]\n",
      "346 [D loss: 1.552526, acc.: 56.25%, op_acc: 75.00%] [G loss: 5.485464]\n",
      "347 [D loss: 1.397014, acc.: 53.12%, op_acc: 78.12%] [G loss: 5.667373]\n",
      "348 [D loss: 1.459200, acc.: 59.38%, op_acc: 71.88%] [G loss: 6.371339]\n",
      "349 [D loss: 1.532380, acc.: 60.94%, op_acc: 73.44%] [G loss: 6.900336]\n",
      "350 [D loss: 1.349609, acc.: 70.31%, op_acc: 78.12%] [G loss: 6.025127]\n",
      "351 [D loss: 1.298836, acc.: 73.44%, op_acc: 78.12%] [G loss: 5.295073]\n",
      "352 [D loss: 1.133651, acc.: 71.88%, op_acc: 81.25%] [G loss: 5.776675]\n",
      "353 [D loss: 1.431153, acc.: 54.69%, op_acc: 81.25%] [G loss: 5.618238]\n",
      "354 [D loss: 1.823020, acc.: 53.12%, op_acc: 67.19%] [G loss: 4.760961]\n",
      "355 [D loss: 1.724013, acc.: 53.12%, op_acc: 67.19%] [G loss: 5.596807]\n",
      "356 [D loss: 1.772888, acc.: 56.25%, op_acc: 71.88%] [G loss: 5.309448]\n",
      "357 [D loss: 1.794667, acc.: 51.56%, op_acc: 70.31%] [G loss: 6.318978]\n",
      "358 [D loss: 1.563496, acc.: 51.56%, op_acc: 79.69%] [G loss: 6.020471]\n",
      "359 [D loss: 1.883046, acc.: 39.06%, op_acc: 67.19%] [G loss: 5.739737]\n",
      "360 [D loss: 1.776720, acc.: 48.44%, op_acc: 67.19%] [G loss: 6.089390]\n",
      "361 [D loss: 1.847595, acc.: 46.88%, op_acc: 64.06%] [G loss: 6.540268]\n",
      "362 [D loss: 1.815677, acc.: 53.12%, op_acc: 71.88%] [G loss: 6.093745]\n",
      "363 [D loss: 1.725515, acc.: 45.31%, op_acc: 68.75%] [G loss: 5.710658]\n",
      "364 [D loss: 1.740136, acc.: 53.12%, op_acc: 70.31%] [G loss: 6.211494]\n",
      "365 [D loss: 1.555820, acc.: 51.56%, op_acc: 67.19%] [G loss: 6.657875]\n",
      "366 [D loss: 1.230433, acc.: 67.19%, op_acc: 79.69%] [G loss: 6.659781]\n",
      "367 [D loss: 1.459458, acc.: 62.50%, op_acc: 75.00%] [G loss: 6.139158]\n",
      "368 [D loss: 1.431837, acc.: 64.06%, op_acc: 68.75%] [G loss: 5.459075]\n",
      "369 [D loss: 1.763204, acc.: 48.44%, op_acc: 71.88%] [G loss: 5.148455]\n",
      "370 [D loss: 1.350062, acc.: 68.75%, op_acc: 76.56%] [G loss: 5.198010]\n",
      "371 [D loss: 1.591036, acc.: 50.00%, op_acc: 85.94%] [G loss: 5.224755]\n",
      "372 [D loss: 1.720500, acc.: 57.81%, op_acc: 65.62%] [G loss: 5.522114]\n",
      "373 [D loss: 1.984597, acc.: 48.44%, op_acc: 64.06%] [G loss: 4.699029]\n",
      "374 [D loss: 1.476993, acc.: 51.56%, op_acc: 76.56%] [G loss: 4.918198]\n",
      "375 [D loss: 1.700625, acc.: 57.81%, op_acc: 79.69%] [G loss: 5.677886]\n",
      "376 [D loss: 1.691217, acc.: 51.56%, op_acc: 62.50%] [G loss: 4.866216]\n",
      "377 [D loss: 1.598083, acc.: 45.31%, op_acc: 75.00%] [G loss: 5.226090]\n",
      "378 [D loss: 1.418873, acc.: 48.44%, op_acc: 82.81%] [G loss: 5.148638]\n",
      "379 [D loss: 1.829541, acc.: 60.94%, op_acc: 71.88%] [G loss: 5.064674]\n",
      "380 [D loss: 1.886021, acc.: 51.56%, op_acc: 67.19%] [G loss: 5.312136]\n",
      "381 [D loss: 1.630157, acc.: 64.06%, op_acc: 75.00%] [G loss: 5.155445]\n",
      "382 [D loss: 1.435907, acc.: 60.94%, op_acc: 73.44%] [G loss: 5.137376]\n",
      "383 [D loss: 1.273691, acc.: 50.00%, op_acc: 73.44%] [G loss: 4.747048]\n",
      "384 [D loss: 1.451913, acc.: 64.06%, op_acc: 75.00%] [G loss: 5.979838]\n",
      "385 [D loss: 1.835953, acc.: 46.88%, op_acc: 70.31%] [G loss: 5.285999]\n",
      "386 [D loss: 1.768865, acc.: 54.69%, op_acc: 71.88%] [G loss: 5.169653]\n",
      "387 [D loss: 1.551021, acc.: 56.25%, op_acc: 76.56%] [G loss: 5.071249]\n",
      "388 [D loss: 1.701041, acc.: 43.75%, op_acc: 64.06%] [G loss: 4.402790]\n",
      "389 [D loss: 1.778535, acc.: 53.12%, op_acc: 67.19%] [G loss: 4.658935]\n",
      "390 [D loss: 1.540461, acc.: 54.69%, op_acc: 78.12%] [G loss: 4.313801]\n",
      "391 [D loss: 1.255219, acc.: 70.31%, op_acc: 76.56%] [G loss: 5.224857]\n",
      "392 [D loss: 1.649907, acc.: 54.69%, op_acc: 67.19%] [G loss: 4.699226]\n",
      "393 [D loss: 1.582569, acc.: 46.88%, op_acc: 70.31%] [G loss: 4.550340]\n",
      "394 [D loss: 1.501325, acc.: 68.75%, op_acc: 75.00%] [G loss: 4.877146]\n",
      "395 [D loss: 1.716968, acc.: 45.31%, op_acc: 64.06%] [G loss: 5.179570]\n",
      "396 [D loss: 1.588070, acc.: 54.69%, op_acc: 68.75%] [G loss: 4.094296]\n",
      "397 [D loss: 1.703638, acc.: 50.00%, op_acc: 67.19%] [G loss: 4.922217]\n",
      "398 [D loss: 1.565680, acc.: 60.94%, op_acc: 70.31%] [G loss: 4.564493]\n",
      "399 [D loss: 1.443493, acc.: 60.94%, op_acc: 70.31%] [G loss: 4.468359]\n",
      "400 [D loss: 1.540912, acc.: 64.06%, op_acc: 64.06%] [G loss: 4.832935]\n",
      "401 [D loss: 1.973603, acc.: 45.31%, op_acc: 57.81%] [G loss: 4.708843]\n",
      "402 [D loss: 1.788934, acc.: 53.12%, op_acc: 62.50%] [G loss: 3.957739]\n",
      "403 [D loss: 1.599200, acc.: 54.69%, op_acc: 65.62%] [G loss: 4.380817]\n",
      "404 [D loss: 1.974575, acc.: 42.19%, op_acc: 57.81%] [G loss: 4.018979]\n",
      "405 [D loss: 2.076321, acc.: 35.94%, op_acc: 67.19%] [G loss: 4.890108]\n",
      "406 [D loss: 1.816688, acc.: 37.50%, op_acc: 68.75%] [G loss: 3.430609]\n",
      "407 [D loss: 1.391673, acc.: 64.06%, op_acc: 76.56%] [G loss: 4.830659]\n",
      "408 [D loss: 2.141380, acc.: 42.19%, op_acc: 56.25%] [G loss: 4.016464]\n",
      "409 [D loss: 1.634536, acc.: 54.69%, op_acc: 65.62%] [G loss: 4.744338]\n",
      "410 [D loss: 1.696999, acc.: 46.88%, op_acc: 65.62%] [G loss: 4.814574]\n",
      "411 [D loss: 1.646899, acc.: 51.56%, op_acc: 71.88%] [G loss: 5.095498]\n",
      "412 [D loss: 1.751654, acc.: 57.81%, op_acc: 53.12%] [G loss: 4.883503]\n",
      "413 [D loss: 1.773051, acc.: 42.19%, op_acc: 59.38%] [G loss: 4.841469]\n",
      "414 [D loss: 1.756276, acc.: 57.81%, op_acc: 71.88%] [G loss: 4.948688]\n",
      "415 [D loss: 1.678219, acc.: 40.62%, op_acc: 68.75%] [G loss: 4.950346]\n",
      "416 [D loss: 1.336598, acc.: 60.94%, op_acc: 73.44%] [G loss: 4.437294]\n",
      "417 [D loss: 1.615596, acc.: 67.19%, op_acc: 67.19%] [G loss: 4.457876]\n",
      "418 [D loss: 1.528505, acc.: 54.69%, op_acc: 73.44%] [G loss: 5.009679]\n",
      "419 [D loss: 1.435336, acc.: 60.94%, op_acc: 71.88%] [G loss: 4.912779]\n",
      "420 [D loss: 1.656430, acc.: 62.50%, op_acc: 67.19%] [G loss: 4.859684]\n",
      "421 [D loss: 1.401332, acc.: 67.19%, op_acc: 78.12%] [G loss: 5.466753]\n",
      "422 [D loss: 1.333706, acc.: 68.75%, op_acc: 78.12%] [G loss: 5.029600]\n",
      "423 [D loss: 1.523866, acc.: 70.31%, op_acc: 73.44%] [G loss: 4.602253]\n",
      "424 [D loss: 1.445477, acc.: 64.06%, op_acc: 78.12%] [G loss: 4.107148]\n",
      "425 [D loss: 1.495710, acc.: 50.00%, op_acc: 75.00%] [G loss: 4.519979]\n",
      "426 [D loss: 1.566083, acc.: 59.38%, op_acc: 65.62%] [G loss: 4.065541]\n",
      "427 [D loss: 1.496685, acc.: 56.25%, op_acc: 75.00%] [G loss: 4.220415]\n",
      "428 [D loss: 1.292130, acc.: 45.31%, op_acc: 79.69%] [G loss: 4.722816]\n",
      "429 [D loss: 1.272546, acc.: 64.06%, op_acc: 75.00%] [G loss: 5.183826]\n",
      "430 [D loss: 1.453801, acc.: 54.69%, op_acc: 73.44%] [G loss: 5.237747]\n",
      "431 [D loss: 1.581606, acc.: 59.38%, op_acc: 75.00%] [G loss: 4.867610]\n",
      "432 [D loss: 1.397148, acc.: 57.81%, op_acc: 76.56%] [G loss: 4.322136]\n",
      "433 [D loss: 1.427128, acc.: 56.25%, op_acc: 79.69%] [G loss: 4.394312]\n",
      "434 [D loss: 1.646899, acc.: 67.19%, op_acc: 67.19%] [G loss: 4.569241]\n",
      "435 [D loss: 1.459484, acc.: 50.00%, op_acc: 73.44%] [G loss: 4.916227]\n",
      "436 [D loss: 1.473505, acc.: 60.94%, op_acc: 67.19%] [G loss: 4.817990]\n",
      "437 [D loss: 1.227599, acc.: 78.12%, op_acc: 79.69%] [G loss: 4.949570]\n",
      "438 [D loss: 1.593622, acc.: 50.00%, op_acc: 71.88%] [G loss: 3.931047]\n",
      "439 [D loss: 1.647413, acc.: 65.62%, op_acc: 59.38%] [G loss: 4.984749]\n",
      "440 [D loss: 1.632604, acc.: 64.06%, op_acc: 79.69%] [G loss: 4.647838]\n",
      "441 [D loss: 1.561606, acc.: 64.06%, op_acc: 73.44%] [G loss: 4.444302]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9a4b261f9c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-e6506f88d9ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# generated imgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1329\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acgan.train(epochs=14000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
