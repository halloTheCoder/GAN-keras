{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout, Input, concatenate, Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import losses, optimizers\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIGAN:\n",
    "    def __init__(self):\n",
    "        img_rows = 28\n",
    "        img_cols = 28\n",
    "        img_channels = 1\n",
    "        self.img_shape = (img_rows, img_cols, img_channels)\n",
    "        self.latent_dim = 100\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = optimizers.Adam(lr = 0.0002, beta_1 = 0.5, beta_2 = 0.999)\n",
    "        \n",
    "        # build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss = losses.binary_crossentropy, optimizer = optimizer, metrics = ['accuracy'])\n",
    "        \n",
    "        self.generator = self.build_generator()\n",
    "        self.encoder = self.build_encoder()\n",
    "        \n",
    "        # The part of the bigan that trains the discriminator and encoder\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # Generate image from sampled noise\n",
    "        z = Input(shape = (self.latent_dim, ))\n",
    "        img_ = self.generator(z)\n",
    "        \n",
    "        # encode image\n",
    "        img = Input(shape = self.img_shape)\n",
    "        z_ = self.encoder(img)\n",
    "        \n",
    "        # Latent -> img is fake, and img -> latent is valid\n",
    "        fake = self.discriminator([z, img_])\n",
    "        valid = self.discriminator([z_, img])\n",
    "        \n",
    "        self.bigan_generator = Model(inputs = [z, img], outputs = [fake, valid])\n",
    "        self.bigan_generator.compile(loss = [losses.binary_crossentropy, losses.binary_crossentropy], optimizer = optimizer)\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.latent_dim))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z = model(img)\n",
    "\n",
    "        return Model(img, z)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        gen_img = model(z)\n",
    "\n",
    "        return Model(z, gen_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img = Input(shape=self.img_shape)\n",
    "        d_in = concatenate([z, Flatten()(img)])\n",
    "\n",
    "        model = Dense(1024)(d_in)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        validity = Dense(1, activation=\"sigmoid\")(model)\n",
    "\n",
    "        return Model([z, img], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Sample noise and generate img\n",
    "            z = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "            imgs_ = self.generator.predict(z)\n",
    "\n",
    "            # Select a random batch of images and encode\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            z_ = self.encoder.predict(imgs)\n",
    "            \n",
    "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
    "            d_loss_real = self.discriminator.train_on_batch([z_, imgs], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (z -> img is valid and img -> z is is invalid)\n",
    "            g_loss = self.bigan_generator.train_on_batch([z, imgs], [valid, fake])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_interval(epoch)\n",
    "\n",
    "    def sample_interval(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        z = np.random.normal(size=(25, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(z)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 720,656\n",
      "Trainable params: 718,608\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 717,924\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bigan = BIGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.209561, acc: 29.69%] [G loss: 3.521557]\n",
      "1 [D loss: 0.537124, acc: 68.75%] [G loss: 4.312933]\n",
      "2 [D loss: 0.297156, acc: 85.94%] [G loss: 5.388489]\n",
      "3 [D loss: 0.145218, acc: 98.44%] [G loss: 5.820424]\n",
      "4 [D loss: 0.124021, acc: 98.44%] [G loss: 6.782402]\n",
      "5 [D loss: 0.069717, acc: 100.00%] [G loss: 7.288142]\n",
      "6 [D loss: 0.052402, acc: 100.00%] [G loss: 8.190542]\n",
      "7 [D loss: 0.044468, acc: 100.00%] [G loss: 7.759377]\n",
      "8 [D loss: 0.046543, acc: 100.00%] [G loss: 8.893176]\n",
      "9 [D loss: 0.028342, acc: 100.00%] [G loss: 10.106620]\n",
      "10 [D loss: 0.028409, acc: 100.00%] [G loss: 10.295261]\n",
      "11 [D loss: 0.021820, acc: 100.00%] [G loss: 10.885851]\n",
      "12 [D loss: 0.024410, acc: 100.00%] [G loss: 11.563570]\n",
      "13 [D loss: 0.010985, acc: 100.00%] [G loss: 11.019537]\n",
      "14 [D loss: 0.010540, acc: 100.00%] [G loss: 11.423648]\n",
      "15 [D loss: 0.014244, acc: 100.00%] [G loss: 11.831979]\n",
      "16 [D loss: 0.010784, acc: 100.00%] [G loss: 12.134274]\n",
      "17 [D loss: 0.008301, acc: 100.00%] [G loss: 11.608919]\n",
      "18 [D loss: 0.012441, acc: 100.00%] [G loss: 13.197432]\n",
      "19 [D loss: 0.008776, acc: 100.00%] [G loss: 13.172283]\n",
      "20 [D loss: 0.010527, acc: 100.00%] [G loss: 13.391444]\n",
      "21 [D loss: 0.008173, acc: 100.00%] [G loss: 13.488025]\n",
      "22 [D loss: 0.004691, acc: 100.00%] [G loss: 14.186025]\n",
      "23 [D loss: 0.062570, acc: 96.88%] [G loss: 16.356401]\n",
      "24 [D loss: 0.006561, acc: 100.00%] [G loss: 16.442043]\n",
      "25 [D loss: 0.015909, acc: 100.00%] [G loss: 17.505840]\n",
      "26 [D loss: 0.010051, acc: 100.00%] [G loss: 15.088100]\n",
      "27 [D loss: 0.007445, acc: 100.00%] [G loss: 16.435583]\n",
      "28 [D loss: 0.029529, acc: 98.44%] [G loss: 17.356224]\n",
      "29 [D loss: 0.008937, acc: 100.00%] [G loss: 17.964199]\n",
      "30 [D loss: 0.005327, acc: 100.00%] [G loss: 18.337608]\n",
      "31 [D loss: 0.006568, acc: 100.00%] [G loss: 17.781696]\n",
      "32 [D loss: 0.007417, acc: 100.00%] [G loss: 18.673244]\n",
      "33 [D loss: 0.006508, acc: 100.00%] [G loss: 18.240494]\n",
      "34 [D loss: 0.007986, acc: 100.00%] [G loss: 18.345795]\n",
      "35 [D loss: 0.005163, acc: 100.00%] [G loss: 19.005972]\n",
      "36 [D loss: 0.005671, acc: 100.00%] [G loss: 17.887568]\n",
      "37 [D loss: 0.003311, acc: 100.00%] [G loss: 18.323044]\n",
      "38 [D loss: 0.002404, acc: 100.00%] [G loss: 18.369919]\n",
      "39 [D loss: 0.004569, acc: 100.00%] [G loss: 18.144211]\n",
      "40 [D loss: 0.003880, acc: 100.00%] [G loss: 17.508556]\n",
      "41 [D loss: 0.081919, acc: 96.88%] [G loss: 20.571169]\n",
      "42 [D loss: 0.009077, acc: 100.00%] [G loss: 20.730759]\n",
      "43 [D loss: 0.019010, acc: 100.00%] [G loss: 21.175781]\n",
      "44 [D loss: 0.007832, acc: 100.00%] [G loss: 21.516241]\n",
      "45 [D loss: 0.006747, acc: 100.00%] [G loss: 20.845730]\n",
      "46 [D loss: 0.010580, acc: 100.00%] [G loss: 20.952814]\n",
      "47 [D loss: 0.005616, acc: 100.00%] [G loss: 21.060715]\n",
      "48 [D loss: 0.007019, acc: 100.00%] [G loss: 21.063917]\n",
      "49 [D loss: 0.013602, acc: 100.00%] [G loss: 21.703663]\n",
      "50 [D loss: 0.082262, acc: 98.44%] [G loss: 21.777882]\n",
      "51 [D loss: 0.012040, acc: 100.00%] [G loss: 21.697636]\n",
      "52 [D loss: 0.007840, acc: 100.00%] [G loss: 21.156717]\n",
      "53 [D loss: 0.016860, acc: 100.00%] [G loss: 22.361324]\n",
      "54 [D loss: 0.329140, acc: 95.31%] [G loss: 21.503922]\n",
      "55 [D loss: 0.093992, acc: 96.88%] [G loss: 21.730879]\n",
      "56 [D loss: 0.034713, acc: 100.00%] [G loss: 21.929306]\n",
      "57 [D loss: 0.004235, acc: 100.00%] [G loss: 22.902668]\n",
      "58 [D loss: 0.012039, acc: 100.00%] [G loss: 22.281178]\n",
      "59 [D loss: 0.037869, acc: 98.44%] [G loss: 22.636070]\n",
      "60 [D loss: 0.014164, acc: 98.44%] [G loss: 23.095701]\n",
      "61 [D loss: 0.001853, acc: 100.00%] [G loss: 22.963703]\n",
      "62 [D loss: 0.010375, acc: 100.00%] [G loss: 23.687231]\n",
      "63 [D loss: 0.005841, acc: 100.00%] [G loss: 22.161907]\n",
      "64 [D loss: 0.058199, acc: 98.44%] [G loss: 22.549427]\n",
      "65 [D loss: 0.004218, acc: 100.00%] [G loss: 22.193737]\n",
      "66 [D loss: 0.004966, acc: 100.00%] [G loss: 22.618792]\n",
      "67 [D loss: 0.004921, acc: 100.00%] [G loss: 22.262581]\n",
      "68 [D loss: 0.012177, acc: 100.00%] [G loss: 22.851395]\n",
      "69 [D loss: 0.051297, acc: 98.44%] [G loss: 20.524456]\n",
      "70 [D loss: 0.295492, acc: 90.62%] [G loss: 21.212803]\n",
      "71 [D loss: 0.073272, acc: 95.31%] [G loss: 22.503803]\n",
      "72 [D loss: 0.047468, acc: 96.88%] [G loss: 23.271336]\n",
      "73 [D loss: 0.019863, acc: 98.44%] [G loss: 23.826899]\n",
      "74 [D loss: 0.053149, acc: 96.88%] [G loss: 24.693752]\n",
      "75 [D loss: 0.037303, acc: 98.44%] [G loss: 24.763966]\n",
      "76 [D loss: 0.222149, acc: 93.75%] [G loss: 23.841396]\n",
      "77 [D loss: 0.028838, acc: 98.44%] [G loss: 24.004669]\n",
      "78 [D loss: 0.604523, acc: 92.19%] [G loss: 23.866779]\n",
      "79 [D loss: 0.398243, acc: 93.75%] [G loss: 23.732840]\n",
      "80 [D loss: 0.747883, acc: 89.06%] [G loss: 21.915232]\n",
      "81 [D loss: 0.730151, acc: 84.38%] [G loss: 22.341440]\n",
      "82 [D loss: 0.839735, acc: 82.81%] [G loss: 24.847095]\n",
      "83 [D loss: 0.662822, acc: 82.81%] [G loss: 23.299824]\n",
      "84 [D loss: 0.073721, acc: 96.88%] [G loss: 17.131969]\n",
      "85 [D loss: 3.877794, acc: 56.25%] [G loss: 22.537863]\n",
      "86 [D loss: 1.939221, acc: 73.44%] [G loss: 22.857325]\n",
      "87 [D loss: 0.265188, acc: 93.75%] [G loss: 21.461922]\n",
      "88 [D loss: 1.888107, acc: 79.69%] [G loss: 22.594410]\n",
      "89 [D loss: 0.477595, acc: 85.94%] [G loss: 22.933861]\n",
      "90 [D loss: 0.288600, acc: 92.19%] [G loss: 18.937695]\n",
      "91 [D loss: 0.505332, acc: 87.50%] [G loss: 20.647629]\n",
      "92 [D loss: 0.476575, acc: 79.69%] [G loss: 23.497627]\n",
      "93 [D loss: 0.281999, acc: 87.50%] [G loss: 19.280296]\n",
      "94 [D loss: 0.565398, acc: 87.50%] [G loss: 18.061962]\n",
      "95 [D loss: 0.477027, acc: 84.38%] [G loss: 19.880846]\n",
      "96 [D loss: 0.103746, acc: 96.88%] [G loss: 17.116590]\n",
      "97 [D loss: 0.319147, acc: 87.50%] [G loss: 16.038845]\n",
      "98 [D loss: 0.511859, acc: 81.25%] [G loss: 20.594606]\n",
      "99 [D loss: 0.917196, acc: 78.12%] [G loss: 18.954042]\n",
      "100 [D loss: 0.171755, acc: 93.75%] [G loss: 13.593384]\n",
      "101 [D loss: 0.734440, acc: 71.88%] [G loss: 20.273144]\n",
      "102 [D loss: 0.575397, acc: 85.94%] [G loss: 17.958469]\n",
      "103 [D loss: 0.116962, acc: 93.75%] [G loss: 14.567919]\n",
      "104 [D loss: 1.129145, acc: 73.44%] [G loss: 22.563910]\n",
      "105 [D loss: 0.614314, acc: 82.81%] [G loss: 16.747269]\n",
      "106 [D loss: 0.642384, acc: 76.56%] [G loss: 16.172855]\n",
      "107 [D loss: 0.263009, acc: 90.62%] [G loss: 11.849012]\n",
      "108 [D loss: 0.446174, acc: 84.38%] [G loss: 16.951530]\n",
      "109 [D loss: 0.527934, acc: 78.12%] [G loss: 14.644498]\n",
      "110 [D loss: 0.547624, acc: 73.44%] [G loss: 12.807775]\n",
      "111 [D loss: 0.323919, acc: 89.06%] [G loss: 11.288997]\n",
      "112 [D loss: 0.553659, acc: 76.56%] [G loss: 16.241348]\n",
      "113 [D loss: 0.568175, acc: 81.25%] [G loss: 14.177450]\n",
      "114 [D loss: 0.232258, acc: 85.94%] [G loss: 11.240879]\n",
      "115 [D loss: 0.523395, acc: 76.56%] [G loss: 15.034952]\n",
      "116 [D loss: 0.455336, acc: 85.94%] [G loss: 12.484512]\n",
      "117 [D loss: 0.212787, acc: 85.94%] [G loss: 11.054930]\n",
      "118 [D loss: 0.549198, acc: 73.44%] [G loss: 14.718702]\n",
      "119 [D loss: 0.287027, acc: 84.38%] [G loss: 10.802113]\n",
      "120 [D loss: 0.322106, acc: 84.38%] [G loss: 12.412776]\n",
      "121 [D loss: 0.136779, acc: 95.31%] [G loss: 11.696556]\n",
      "122 [D loss: 0.519429, acc: 75.00%] [G loss: 12.184711]\n",
      "123 [D loss: 0.275229, acc: 87.50%] [G loss: 10.912484]\n",
      "124 [D loss: 0.413148, acc: 78.12%] [G loss: 11.212969]\n",
      "125 [D loss: 0.382747, acc: 75.00%] [G loss: 10.847155]\n",
      "126 [D loss: 0.442637, acc: 78.12%] [G loss: 11.404148]\n",
      "127 [D loss: 0.241164, acc: 82.81%] [G loss: 10.967573]\n",
      "128 [D loss: 0.416474, acc: 79.69%] [G loss: 11.838530]\n",
      "129 [D loss: 0.440428, acc: 79.69%] [G loss: 9.432503]\n",
      "130 [D loss: 0.269448, acc: 84.38%] [G loss: 11.114630]\n",
      "131 [D loss: 0.391502, acc: 81.25%] [G loss: 9.443588]\n",
      "132 [D loss: 0.335891, acc: 87.50%] [G loss: 9.745832]\n",
      "133 [D loss: 0.389623, acc: 76.56%] [G loss: 9.696661]\n",
      "134 [D loss: 0.405654, acc: 81.25%] [G loss: 9.861778]\n",
      "135 [D loss: 0.280319, acc: 81.25%] [G loss: 9.897099]\n",
      "136 [D loss: 0.444739, acc: 68.75%] [G loss: 9.118383]\n",
      "137 [D loss: 0.200635, acc: 90.62%] [G loss: 9.686361]\n",
      "138 [D loss: 0.188509, acc: 92.19%] [G loss: 9.860533]\n",
      "139 [D loss: 0.364834, acc: 82.81%] [G loss: 10.773553]\n",
      "140 [D loss: 0.367623, acc: 81.25%] [G loss: 10.132460]\n",
      "141 [D loss: 0.241589, acc: 85.94%] [G loss: 9.151873]\n",
      "142 [D loss: 0.555601, acc: 67.19%] [G loss: 12.361773]\n",
      "143 [D loss: 0.367333, acc: 84.38%] [G loss: 10.770855]\n",
      "144 [D loss: 0.348786, acc: 82.81%] [G loss: 9.430711]\n",
      "145 [D loss: 0.329693, acc: 82.81%] [G loss: 9.705460]\n",
      "146 [D loss: 0.391836, acc: 79.69%] [G loss: 9.534420]\n",
      "147 [D loss: 0.298100, acc: 84.38%] [G loss: 9.609770]\n",
      "148 [D loss: 0.247483, acc: 87.50%] [G loss: 9.488676]\n",
      "149 [D loss: 0.240368, acc: 87.50%] [G loss: 10.219274]\n",
      "150 [D loss: 0.238827, acc: 84.38%] [G loss: 9.829400]\n",
      "151 [D loss: 0.326880, acc: 81.25%] [G loss: 9.576625]\n",
      "152 [D loss: 0.308675, acc: 87.50%] [G loss: 9.664391]\n",
      "153 [D loss: 0.422811, acc: 78.12%] [G loss: 10.590278]\n",
      "154 [D loss: 0.270063, acc: 84.38%] [G loss: 9.071997]\n",
      "155 [D loss: 0.343800, acc: 81.25%] [G loss: 8.623306]\n",
      "156 [D loss: 0.214919, acc: 89.06%] [G loss: 9.564095]\n",
      "157 [D loss: 0.306878, acc: 82.81%] [G loss: 10.620583]\n",
      "158 [D loss: 0.271838, acc: 85.94%] [G loss: 9.586147]\n",
      "159 [D loss: 0.237193, acc: 92.19%] [G loss: 9.905273]\n",
      "160 [D loss: 0.467243, acc: 75.00%] [G loss: 11.198689]\n",
      "161 [D loss: 0.343332, acc: 82.81%] [G loss: 9.818637]\n",
      "162 [D loss: 0.193770, acc: 89.06%] [G loss: 9.358150]\n",
      "163 [D loss: 0.499582, acc: 79.69%] [G loss: 11.224522]\n",
      "164 [D loss: 0.346838, acc: 79.69%] [G loss: 8.918749]\n",
      "165 [D loss: 0.219629, acc: 89.06%] [G loss: 8.272539]\n",
      "166 [D loss: 0.306273, acc: 79.69%] [G loss: 10.307962]\n",
      "167 [D loss: 0.210103, acc: 87.50%] [G loss: 9.681827]\n",
      "168 [D loss: 0.189246, acc: 89.06%] [G loss: 8.232981]\n",
      "169 [D loss: 0.249611, acc: 87.50%] [G loss: 10.156694]\n",
      "170 [D loss: 0.333670, acc: 85.94%] [G loss: 9.333742]\n",
      "171 [D loss: 0.253370, acc: 87.50%] [G loss: 8.039085]\n",
      "172 [D loss: 0.175742, acc: 92.19%] [G loss: 10.306509]\n",
      "173 [D loss: 0.241734, acc: 85.94%] [G loss: 9.161438]\n",
      "174 [D loss: 0.444172, acc: 71.88%] [G loss: 8.578181]\n",
      "175 [D loss: 0.175810, acc: 93.75%] [G loss: 9.152873]\n",
      "176 [D loss: 0.328168, acc: 85.94%] [G loss: 8.494949]\n",
      "177 [D loss: 0.207321, acc: 92.19%] [G loss: 9.996315]\n",
      "178 [D loss: 0.211661, acc: 90.62%] [G loss: 8.650507]\n",
      "179 [D loss: 0.583740, acc: 67.19%] [G loss: 10.372961]\n",
      "180 [D loss: 0.194852, acc: 87.50%] [G loss: 8.222480]\n",
      "181 [D loss: 0.563298, acc: 68.75%] [G loss: 9.636950]\n",
      "182 [D loss: 0.229833, acc: 87.50%] [G loss: 9.638468]\n",
      "183 [D loss: 0.357178, acc: 81.25%] [G loss: 8.171926]\n",
      "184 [D loss: 0.263811, acc: 87.50%] [G loss: 8.685013]\n",
      "185 [D loss: 0.221013, acc: 89.06%] [G loss: 8.902889]\n",
      "186 [D loss: 0.193916, acc: 92.19%] [G loss: 8.394388]\n",
      "187 [D loss: 0.263249, acc: 87.50%] [G loss: 8.769176]\n",
      "188 [D loss: 0.266458, acc: 84.38%] [G loss: 9.264805]\n",
      "189 [D loss: 0.191187, acc: 92.19%] [G loss: 7.789511]\n",
      "190 [D loss: 0.314962, acc: 84.38%] [G loss: 8.893897]\n",
      "191 [D loss: 0.211180, acc: 89.06%] [G loss: 7.531339]\n",
      "192 [D loss: 0.317729, acc: 82.81%] [G loss: 9.169832]\n",
      "193 [D loss: 0.182577, acc: 90.62%] [G loss: 8.467198]\n",
      "194 [D loss: 0.203701, acc: 90.62%] [G loss: 8.337837]\n",
      "195 [D loss: 0.161230, acc: 93.75%] [G loss: 8.498381]\n",
      "196 [D loss: 0.146539, acc: 92.19%] [G loss: 8.025318]\n",
      "197 [D loss: 0.273962, acc: 85.94%] [G loss: 7.527996]\n",
      "198 [D loss: 0.178644, acc: 96.88%] [G loss: 6.921067]\n",
      "199 [D loss: 0.273590, acc: 89.06%] [G loss: 8.879284]\n",
      "200 [D loss: 0.218241, acc: 85.94%] [G loss: 6.813466]\n",
      "201 [D loss: 0.176987, acc: 95.31%] [G loss: 7.416952]\n",
      "202 [D loss: 0.130292, acc: 95.31%] [G loss: 7.626918]\n",
      "203 [D loss: 0.222878, acc: 90.62%] [G loss: 8.511464]\n",
      "204 [D loss: 0.281030, acc: 85.94%] [G loss: 7.708064]\n",
      "205 [D loss: 0.141400, acc: 96.88%] [G loss: 7.423892]\n",
      "206 [D loss: 0.238951, acc: 89.06%] [G loss: 6.575131]\n",
      "207 [D loss: 0.217852, acc: 95.31%] [G loss: 7.255711]\n",
      "208 [D loss: 0.189330, acc: 92.19%] [G loss: 6.786767]\n",
      "209 [D loss: 0.230296, acc: 93.75%] [G loss: 8.102436]\n",
      "210 [D loss: 0.211609, acc: 89.06%] [G loss: 7.721187]\n",
      "211 [D loss: 0.256999, acc: 85.94%] [G loss: 7.836900]\n",
      "212 [D loss: 0.323022, acc: 85.94%] [G loss: 7.147252]\n",
      "213 [D loss: 0.147771, acc: 95.31%] [G loss: 6.407187]\n",
      "214 [D loss: 0.273059, acc: 87.50%] [G loss: 7.744514]\n",
      "215 [D loss: 0.335168, acc: 82.81%] [G loss: 8.501873]\n",
      "216 [D loss: 0.212213, acc: 90.62%] [G loss: 7.212114]\n",
      "217 [D loss: 0.121933, acc: 96.88%] [G loss: 7.037745]\n",
      "218 [D loss: 0.264673, acc: 93.75%] [G loss: 7.452133]\n",
      "219 [D loss: 0.329628, acc: 81.25%] [G loss: 6.682092]\n",
      "220 [D loss: 0.322542, acc: 82.81%] [G loss: 7.288899]\n",
      "221 [D loss: 0.313461, acc: 84.38%] [G loss: 6.767815]\n",
      "222 [D loss: 0.369287, acc: 89.06%] [G loss: 6.897376]\n",
      "223 [D loss: 0.207900, acc: 92.19%] [G loss: 6.619792]\n",
      "224 [D loss: 0.354171, acc: 79.69%] [G loss: 6.008949]\n",
      "225 [D loss: 0.389942, acc: 79.69%] [G loss: 7.208035]\n",
      "226 [D loss: 0.207505, acc: 90.62%] [G loss: 5.216300]\n",
      "227 [D loss: 0.205812, acc: 93.75%] [G loss: 6.388016]\n",
      "228 [D loss: 0.210434, acc: 89.06%] [G loss: 6.820963]\n",
      "229 [D loss: 0.159521, acc: 98.44%] [G loss: 5.731340]\n",
      "230 [D loss: 0.538486, acc: 71.88%] [G loss: 6.354403]\n",
      "231 [D loss: 0.222730, acc: 89.06%] [G loss: 5.076921]\n",
      "232 [D loss: 0.446324, acc: 82.81%] [G loss: 6.586733]\n",
      "233 [D loss: 0.263941, acc: 85.94%] [G loss: 6.883463]\n",
      "234 [D loss: 0.217843, acc: 93.75%] [G loss: 6.611049]\n",
      "235 [D loss: 0.147649, acc: 98.44%] [G loss: 5.965222]\n",
      "236 [D loss: 0.151114, acc: 96.88%] [G loss: 5.585670]\n",
      "237 [D loss: 0.410612, acc: 85.94%] [G loss: 5.434088]\n",
      "238 [D loss: 0.372323, acc: 81.25%] [G loss: 5.438823]\n",
      "239 [D loss: 0.272732, acc: 87.50%] [G loss: 6.149028]\n",
      "240 [D loss: 0.275874, acc: 87.50%] [G loss: 6.285905]\n",
      "241 [D loss: 0.297089, acc: 82.81%] [G loss: 5.183949]\n",
      "242 [D loss: 0.495001, acc: 79.69%] [G loss: 6.305239]\n",
      "243 [D loss: 0.338379, acc: 84.38%] [G loss: 6.273865]\n",
      "244 [D loss: 0.311511, acc: 84.38%] [G loss: 6.059489]\n",
      "245 [D loss: 0.299894, acc: 89.06%] [G loss: 5.675080]\n",
      "246 [D loss: 0.244177, acc: 90.62%] [G loss: 5.962765]\n",
      "247 [D loss: 0.218619, acc: 92.19%] [G loss: 6.148679]\n",
      "248 [D loss: 0.278366, acc: 85.94%] [G loss: 5.888721]\n",
      "249 [D loss: 0.410442, acc: 84.38%] [G loss: 5.581668]\n",
      "250 [D loss: 0.214967, acc: 90.62%] [G loss: 5.523413]\n",
      "251 [D loss: 0.529815, acc: 67.19%] [G loss: 5.625258]\n",
      "252 [D loss: 0.201933, acc: 92.19%] [G loss: 5.186198]\n",
      "253 [D loss: 0.307467, acc: 89.06%] [G loss: 4.876910]\n",
      "254 [D loss: 0.297243, acc: 89.06%] [G loss: 5.426694]\n",
      "255 [D loss: 0.145129, acc: 93.75%] [G loss: 4.778899]\n",
      "256 [D loss: 0.460280, acc: 76.56%] [G loss: 6.295164]\n",
      "257 [D loss: 0.321835, acc: 79.69%] [G loss: 4.683874]\n",
      "258 [D loss: 0.545313, acc: 81.25%] [G loss: 5.996850]\n",
      "259 [D loss: 0.476676, acc: 65.62%] [G loss: 5.124518]\n",
      "260 [D loss: 0.313910, acc: 85.94%] [G loss: 4.627164]\n",
      "261 [D loss: 0.529755, acc: 76.56%] [G loss: 5.602916]\n",
      "262 [D loss: 0.209333, acc: 92.19%] [G loss: 4.722782]\n",
      "263 [D loss: 0.237252, acc: 93.75%] [G loss: 4.301414]\n",
      "264 [D loss: 0.373699, acc: 79.69%] [G loss: 4.565126]\n",
      "265 [D loss: 0.399248, acc: 79.69%] [G loss: 5.084577]\n",
      "266 [D loss: 0.241544, acc: 89.06%] [G loss: 5.226532]\n",
      "267 [D loss: 0.565907, acc: 71.88%] [G loss: 5.775872]\n",
      "268 [D loss: 0.275031, acc: 82.81%] [G loss: 4.522243]\n",
      "269 [D loss: 0.423438, acc: 78.12%] [G loss: 4.704707]\n",
      "270 [D loss: 0.213998, acc: 89.06%] [G loss: 4.143713]\n",
      "271 [D loss: 0.465188, acc: 79.69%] [G loss: 5.960040]\n",
      "272 [D loss: 0.251016, acc: 90.62%] [G loss: 4.430796]\n",
      "273 [D loss: 0.215643, acc: 90.62%] [G loss: 4.127136]\n",
      "274 [D loss: 0.520672, acc: 76.56%] [G loss: 4.692122]\n",
      "275 [D loss: 0.145185, acc: 98.44%] [G loss: 4.654617]\n",
      "276 [D loss: 0.394536, acc: 81.25%] [G loss: 5.687603]\n",
      "277 [D loss: 0.200235, acc: 90.62%] [G loss: 4.483347]\n",
      "278 [D loss: 0.241181, acc: 92.19%] [G loss: 4.841674]\n",
      "279 [D loss: 0.401674, acc: 75.00%] [G loss: 4.981254]\n",
      "280 [D loss: 0.225456, acc: 93.75%] [G loss: 4.593830]\n",
      "281 [D loss: 1.031479, acc: 57.81%] [G loss: 5.575958]\n",
      "282 [D loss: 0.347868, acc: 79.69%] [G loss: 4.344474]\n",
      "283 [D loss: 0.594765, acc: 62.50%] [G loss: 5.432908]\n",
      "284 [D loss: 0.157829, acc: 98.44%] [G loss: 3.872933]\n",
      "285 [D loss: 0.416988, acc: 82.81%] [G loss: 4.462819]\n",
      "286 [D loss: 0.241888, acc: 90.62%] [G loss: 4.347239]\n",
      "287 [D loss: 0.326590, acc: 87.50%] [G loss: 5.564492]\n",
      "288 [D loss: 0.263729, acc: 90.62%] [G loss: 4.498536]\n",
      "289 [D loss: 0.717016, acc: 59.38%] [G loss: 5.376350]\n",
      "290 [D loss: 0.198827, acc: 95.31%] [G loss: 4.625913]\n",
      "291 [D loss: 0.333467, acc: 82.81%] [G loss: 4.057917]\n",
      "292 [D loss: 0.298989, acc: 90.62%] [G loss: 4.140572]\n",
      "293 [D loss: 0.488861, acc: 75.00%] [G loss: 5.009685]\n",
      "294 [D loss: 0.279287, acc: 87.50%] [G loss: 4.581145]\n",
      "295 [D loss: 0.285124, acc: 90.62%] [G loss: 4.459755]\n",
      "296 [D loss: 0.434085, acc: 76.56%] [G loss: 5.038515]\n",
      "297 [D loss: 0.286094, acc: 87.50%] [G loss: 4.213127]\n",
      "298 [D loss: 0.357327, acc: 84.38%] [G loss: 4.476635]\n",
      "299 [D loss: 0.721535, acc: 59.38%] [G loss: 5.966539]\n",
      "300 [D loss: 0.261556, acc: 85.94%] [G loss: 3.750175]\n",
      "301 [D loss: 0.548961, acc: 70.31%] [G loss: 5.379878]\n",
      "302 [D loss: 0.262596, acc: 90.62%] [G loss: 4.234176]\n",
      "303 [D loss: 0.293341, acc: 87.50%] [G loss: 4.360676]\n",
      "304 [D loss: 0.385220, acc: 82.81%] [G loss: 4.602237]\n",
      "305 [D loss: 0.293298, acc: 92.19%] [G loss: 4.727363]\n",
      "306 [D loss: 0.350401, acc: 85.94%] [G loss: 4.448950]\n",
      "307 [D loss: 0.178374, acc: 98.44%] [G loss: 3.545280]\n",
      "308 [D loss: 0.620520, acc: 67.19%] [G loss: 5.169395]\n",
      "309 [D loss: 0.314508, acc: 85.94%] [G loss: 4.868843]\n",
      "310 [D loss: 0.470312, acc: 75.00%] [G loss: 4.200758]\n",
      "311 [D loss: 0.324409, acc: 89.06%] [G loss: 4.293929]\n",
      "312 [D loss: 0.357389, acc: 85.94%] [G loss: 5.218903]\n",
      "313 [D loss: 0.199720, acc: 95.31%] [G loss: 4.528617]\n",
      "314 [D loss: 0.448383, acc: 81.25%] [G loss: 4.688655]\n",
      "315 [D loss: 0.295210, acc: 84.38%] [G loss: 3.942562]\n",
      "316 [D loss: 0.603711, acc: 73.44%] [G loss: 6.144279]\n",
      "317 [D loss: 0.440368, acc: 70.31%] [G loss: 3.843919]\n",
      "318 [D loss: 0.456387, acc: 76.56%] [G loss: 4.630809]\n",
      "319 [D loss: 0.368038, acc: 85.94%] [G loss: 4.924409]\n",
      "320 [D loss: 0.254508, acc: 89.06%] [G loss: 4.971748]\n",
      "321 [D loss: 0.325446, acc: 87.50%] [G loss: 3.816659]\n",
      "322 [D loss: 0.454586, acc: 75.00%] [G loss: 4.800439]\n",
      "323 [D loss: 0.231830, acc: 93.75%] [G loss: 4.311274]\n",
      "324 [D loss: 0.424920, acc: 81.25%] [G loss: 5.204349]\n",
      "325 [D loss: 0.250829, acc: 92.19%] [G loss: 4.507386]\n",
      "326 [D loss: 0.432247, acc: 79.69%] [G loss: 4.820570]\n",
      "327 [D loss: 0.284455, acc: 87.50%] [G loss: 3.644187]\n",
      "328 [D loss: 0.956451, acc: 53.12%] [G loss: 5.809546]\n",
      "329 [D loss: 0.264375, acc: 87.50%] [G loss: 4.466768]\n",
      "330 [D loss: 0.336570, acc: 89.06%] [G loss: 4.220395]\n",
      "331 [D loss: 0.426564, acc: 82.81%] [G loss: 4.193948]\n",
      "332 [D loss: 0.352282, acc: 84.38%] [G loss: 4.132225]\n",
      "333 [D loss: 0.350752, acc: 92.19%] [G loss: 4.585073]\n",
      "334 [D loss: 0.312927, acc: 87.50%] [G loss: 4.503784]\n",
      "335 [D loss: 0.409114, acc: 79.69%] [G loss: 4.451355]\n",
      "336 [D loss: 0.343963, acc: 82.81%] [G loss: 4.815274]\n",
      "337 [D loss: 0.409588, acc: 78.12%] [G loss: 4.080816]\n",
      "338 [D loss: 0.393339, acc: 85.94%] [G loss: 5.507996]\n",
      "339 [D loss: 0.351672, acc: 84.38%] [G loss: 4.521008]\n",
      "340 [D loss: 0.401479, acc: 79.69%] [G loss: 4.247399]\n",
      "341 [D loss: 0.244794, acc: 96.88%] [G loss: 5.053720]\n",
      "342 [D loss: 0.326146, acc: 87.50%] [G loss: 4.505861]\n",
      "343 [D loss: 0.540775, acc: 76.56%] [G loss: 4.569846]\n",
      "344 [D loss: 0.310450, acc: 89.06%] [G loss: 4.866055]\n",
      "345 [D loss: 0.299210, acc: 90.62%] [G loss: 4.053810]\n",
      "346 [D loss: 0.665791, acc: 68.75%] [G loss: 5.676749]\n",
      "347 [D loss: 0.194768, acc: 96.88%] [G loss: 4.466603]\n",
      "348 [D loss: 0.325205, acc: 84.38%] [G loss: 4.734574]\n",
      "349 [D loss: 0.344166, acc: 82.81%] [G loss: 5.404936]\n",
      "350 [D loss: 0.353010, acc: 84.38%] [G loss: 5.259787]\n",
      "351 [D loss: 0.315991, acc: 82.81%] [G loss: 4.255057]\n",
      "352 [D loss: 0.291673, acc: 95.31%] [G loss: 5.038806]\n",
      "353 [D loss: 0.361006, acc: 87.50%] [G loss: 4.203031]\n",
      "354 [D loss: 0.440763, acc: 82.81%] [G loss: 5.069605]\n",
      "355 [D loss: 0.349526, acc: 84.38%] [G loss: 4.736052]\n",
      "356 [D loss: 0.460272, acc: 75.00%] [G loss: 5.012675]\n",
      "357 [D loss: 0.476640, acc: 79.69%] [G loss: 4.841470]\n",
      "358 [D loss: 0.341480, acc: 85.94%] [G loss: 4.221434]\n",
      "359 [D loss: 0.464908, acc: 76.56%] [G loss: 4.544657]\n",
      "360 [D loss: 0.236316, acc: 92.19%] [G loss: 4.147518]\n",
      "361 [D loss: 0.367270, acc: 81.25%] [G loss: 4.141428]\n",
      "362 [D loss: 0.236228, acc: 93.75%] [G loss: 4.860398]\n",
      "363 [D loss: 0.393002, acc: 81.25%] [G loss: 4.825031]\n",
      "364 [D loss: 0.433710, acc: 84.38%] [G loss: 5.101219]\n",
      "365 [D loss: 0.374701, acc: 85.94%] [G loss: 4.401772]\n",
      "366 [D loss: 0.281134, acc: 84.38%] [G loss: 4.233669]\n",
      "367 [D loss: 0.532031, acc: 70.31%] [G loss: 4.988924]\n",
      "368 [D loss: 0.250375, acc: 93.75%] [G loss: 4.403372]\n",
      "369 [D loss: 0.630675, acc: 67.19%] [G loss: 4.972106]\n",
      "370 [D loss: 0.245968, acc: 92.19%] [G loss: 4.113510]\n",
      "371 [D loss: 0.307910, acc: 87.50%] [G loss: 4.924390]\n",
      "372 [D loss: 0.246980, acc: 92.19%] [G loss: 4.444696]\n",
      "373 [D loss: 0.438020, acc: 81.25%] [G loss: 5.583745]\n",
      "374 [D loss: 0.318838, acc: 84.38%] [G loss: 4.985778]\n",
      "375 [D loss: 0.285101, acc: 90.62%] [G loss: 4.399087]\n",
      "376 [D loss: 0.461273, acc: 78.12%] [G loss: 4.044807]\n",
      "377 [D loss: 0.523759, acc: 75.00%] [G loss: 4.364177]\n",
      "378 [D loss: 0.327920, acc: 89.06%] [G loss: 4.267760]\n",
      "379 [D loss: 0.344365, acc: 84.38%] [G loss: 4.536650]\n",
      "380 [D loss: 0.223603, acc: 93.75%] [G loss: 4.609888]\n",
      "381 [D loss: 0.348394, acc: 87.50%] [G loss: 4.470056]\n",
      "382 [D loss: 0.424231, acc: 81.25%] [G loss: 3.894113]\n",
      "383 [D loss: 0.686424, acc: 68.75%] [G loss: 5.879196]\n",
      "384 [D loss: 0.237319, acc: 93.75%] [G loss: 4.167748]\n",
      "385 [D loss: 0.533900, acc: 71.88%] [G loss: 4.613195]\n",
      "386 [D loss: 0.322314, acc: 87.50%] [G loss: 4.253959]\n",
      "387 [D loss: 0.512577, acc: 76.56%] [G loss: 5.093892]\n",
      "388 [D loss: 0.303122, acc: 89.06%] [G loss: 4.129626]\n",
      "389 [D loss: 0.560823, acc: 73.44%] [G loss: 4.629392]\n",
      "390 [D loss: 0.431738, acc: 82.81%] [G loss: 4.268651]\n",
      "391 [D loss: 0.350261, acc: 84.38%] [G loss: 4.043681]\n",
      "392 [D loss: 0.311865, acc: 92.19%] [G loss: 4.847351]\n",
      "393 [D loss: 0.506947, acc: 67.19%] [G loss: 4.570663]\n",
      "394 [D loss: 0.304953, acc: 90.62%] [G loss: 4.668967]\n",
      "395 [D loss: 0.386368, acc: 84.38%] [G loss: 4.783269]\n",
      "396 [D loss: 0.399594, acc: 79.69%] [G loss: 4.831903]\n",
      "397 [D loss: 0.371213, acc: 81.25%] [G loss: 3.736514]\n",
      "398 [D loss: 0.532601, acc: 76.56%] [G loss: 4.383262]\n",
      "399 [D loss: 0.507700, acc: 78.12%] [G loss: 3.722392]\n",
      "400 [D loss: 0.353141, acc: 90.62%] [G loss: 4.008780]\n",
      "401 [D loss: 0.378786, acc: 84.38%] [G loss: 4.253304]\n",
      "402 [D loss: 0.389258, acc: 79.69%] [G loss: 4.021683]\n",
      "403 [D loss: 0.389212, acc: 84.38%] [G loss: 4.596495]\n",
      "404 [D loss: 0.467539, acc: 79.69%] [G loss: 5.154826]\n",
      "405 [D loss: 0.289485, acc: 90.62%] [G loss: 4.389232]\n",
      "406 [D loss: 0.356643, acc: 84.38%] [G loss: 4.290803]\n",
      "407 [D loss: 0.439730, acc: 81.25%] [G loss: 4.432146]\n",
      "408 [D loss: 0.331824, acc: 85.94%] [G loss: 4.927167]\n",
      "409 [D loss: 0.469010, acc: 79.69%] [G loss: 5.098900]\n",
      "410 [D loss: 0.499028, acc: 73.44%] [G loss: 4.299970]\n",
      "411 [D loss: 0.398344, acc: 84.38%] [G loss: 4.774600]\n",
      "412 [D loss: 0.448117, acc: 78.12%] [G loss: 4.621958]\n",
      "413 [D loss: 0.304606, acc: 87.50%] [G loss: 3.882398]\n",
      "414 [D loss: 0.410472, acc: 84.38%] [G loss: 5.129137]\n",
      "415 [D loss: 0.393066, acc: 85.94%] [G loss: 4.745703]\n",
      "416 [D loss: 0.482105, acc: 79.69%] [G loss: 3.931718]\n",
      "417 [D loss: 0.496839, acc: 76.56%] [G loss: 4.348832]\n",
      "418 [D loss: 0.394768, acc: 81.25%] [G loss: 4.149796]\n",
      "419 [D loss: 0.263059, acc: 92.19%] [G loss: 4.019123]\n",
      "420 [D loss: 0.378640, acc: 84.38%] [G loss: 5.047330]\n",
      "421 [D loss: 0.361121, acc: 81.25%] [G loss: 4.040651]\n",
      "422 [D loss: 0.513696, acc: 79.69%] [G loss: 4.433414]\n",
      "423 [D loss: 0.411073, acc: 76.56%] [G loss: 4.775263]\n",
      "424 [D loss: 0.319891, acc: 85.94%] [G loss: 4.371923]\n",
      "425 [D loss: 0.476478, acc: 78.12%] [G loss: 4.600981]\n",
      "426 [D loss: 0.390419, acc: 87.50%] [G loss: 5.128404]\n",
      "427 [D loss: 0.370503, acc: 82.81%] [G loss: 3.831009]\n",
      "428 [D loss: 0.456488, acc: 84.38%] [G loss: 4.304700]\n",
      "429 [D loss: 0.612562, acc: 67.19%] [G loss: 4.580846]\n",
      "430 [D loss: 0.239751, acc: 93.75%] [G loss: 2.997849]\n",
      "431 [D loss: 0.482812, acc: 79.69%] [G loss: 4.587183]\n",
      "432 [D loss: 0.272030, acc: 87.50%] [G loss: 4.179052]\n",
      "433 [D loss: 0.515150, acc: 73.44%] [G loss: 4.421900]\n",
      "434 [D loss: 0.393465, acc: 79.69%] [G loss: 4.326368]\n",
      "435 [D loss: 0.378846, acc: 81.25%] [G loss: 3.840019]\n",
      "436 [D loss: 0.629240, acc: 71.88%] [G loss: 3.965288]\n",
      "437 [D loss: 0.302430, acc: 87.50%] [G loss: 3.997262]\n",
      "438 [D loss: 0.351565, acc: 90.62%] [G loss: 4.695816]\n",
      "439 [D loss: 0.313270, acc: 92.19%] [G loss: 4.283014]\n",
      "440 [D loss: 0.274064, acc: 90.62%] [G loss: 4.403677]\n",
      "441 [D loss: 0.425148, acc: 78.12%] [G loss: 5.020879]\n",
      "442 [D loss: 0.262268, acc: 92.19%] [G loss: 4.481270]\n",
      "443 [D loss: 0.612535, acc: 67.19%] [G loss: 4.704230]\n",
      "444 [D loss: 0.332864, acc: 85.94%] [G loss: 4.480210]\n",
      "445 [D loss: 0.406190, acc: 84.38%] [G loss: 4.258079]\n",
      "446 [D loss: 0.371044, acc: 85.94%] [G loss: 4.140573]\n",
      "447 [D loss: 0.487999, acc: 78.12%] [G loss: 4.281959]\n",
      "448 [D loss: 0.498217, acc: 78.12%] [G loss: 4.122924]\n",
      "449 [D loss: 0.446125, acc: 81.25%] [G loss: 4.823770]\n",
      "450 [D loss: 0.735461, acc: 57.81%] [G loss: 4.798214]\n",
      "451 [D loss: 0.410046, acc: 73.44%] [G loss: 4.243632]\n",
      "452 [D loss: 0.402189, acc: 89.06%] [G loss: 4.206301]\n",
      "453 [D loss: 0.347978, acc: 81.25%] [G loss: 4.222068]\n",
      "454 [D loss: 0.363589, acc: 82.81%] [G loss: 4.125013]\n",
      "455 [D loss: 0.303096, acc: 89.06%] [G loss: 4.391496]\n",
      "456 [D loss: 0.515926, acc: 73.44%] [G loss: 4.394145]\n",
      "457 [D loss: 0.405498, acc: 84.38%] [G loss: 4.227942]\n",
      "458 [D loss: 0.406085, acc: 85.94%] [G loss: 3.738830]\n",
      "459 [D loss: 0.266960, acc: 93.75%] [G loss: 4.051062]\n",
      "460 [D loss: 0.638527, acc: 64.06%] [G loss: 4.514041]\n",
      "461 [D loss: 0.335230, acc: 85.94%] [G loss: 3.917095]\n",
      "462 [D loss: 0.486320, acc: 78.12%] [G loss: 4.241506]\n",
      "463 [D loss: 0.284223, acc: 90.62%] [G loss: 3.895095]\n",
      "464 [D loss: 0.331285, acc: 90.62%] [G loss: 4.273409]\n",
      "465 [D loss: 0.289520, acc: 90.62%] [G loss: 3.948727]\n",
      "466 [D loss: 0.500724, acc: 70.31%] [G loss: 4.483439]\n",
      "467 [D loss: 0.365577, acc: 84.38%] [G loss: 4.080112]\n",
      "468 [D loss: 0.489895, acc: 78.12%] [G loss: 4.025154]\n",
      "469 [D loss: 0.412307, acc: 85.94%] [G loss: 4.466256]\n",
      "470 [D loss: 0.457077, acc: 78.12%] [G loss: 3.881197]\n",
      "471 [D loss: 0.271956, acc: 90.62%] [G loss: 3.993996]\n",
      "472 [D loss: 0.444404, acc: 75.00%] [G loss: 4.753753]\n",
      "473 [D loss: 0.377225, acc: 82.81%] [G loss: 4.549511]\n",
      "474 [D loss: 0.384470, acc: 82.81%] [G loss: 3.913546]\n",
      "475 [D loss: 0.648472, acc: 64.06%] [G loss: 4.390273]\n",
      "476 [D loss: 0.336142, acc: 85.94%] [G loss: 4.036892]\n",
      "477 [D loss: 0.434491, acc: 78.12%] [G loss: 4.969730]\n",
      "478 [D loss: 0.298849, acc: 92.19%] [G loss: 3.885459]\n",
      "479 [D loss: 0.448448, acc: 75.00%] [G loss: 3.829450]\n",
      "480 [D loss: 0.377074, acc: 84.38%] [G loss: 4.226989]\n",
      "481 [D loss: 0.354647, acc: 81.25%] [G loss: 3.924251]\n",
      "482 [D loss: 0.460690, acc: 78.12%] [G loss: 4.180493]\n",
      "483 [D loss: 0.428788, acc: 84.38%] [G loss: 4.238221]\n",
      "484 [D loss: 0.329960, acc: 84.38%] [G loss: 4.215263]\n",
      "485 [D loss: 0.458611, acc: 76.56%] [G loss: 3.923078]\n",
      "486 [D loss: 0.452128, acc: 81.25%] [G loss: 4.363711]\n",
      "487 [D loss: 0.521405, acc: 75.00%] [G loss: 4.586942]\n",
      "488 [D loss: 0.398143, acc: 82.81%] [G loss: 3.882578]\n",
      "489 [D loss: 0.434851, acc: 76.56%] [G loss: 4.463312]\n",
      "490 [D loss: 0.416203, acc: 82.81%] [G loss: 4.518908]\n",
      "491 [D loss: 0.414668, acc: 87.50%] [G loss: 3.976329]\n",
      "492 [D loss: 0.280297, acc: 90.62%] [G loss: 4.065799]\n",
      "493 [D loss: 0.401302, acc: 82.81%] [G loss: 4.868301]\n",
      "494 [D loss: 0.524081, acc: 67.19%] [G loss: 3.625600]\n",
      "495 [D loss: 0.430525, acc: 82.81%] [G loss: 4.056529]\n",
      "496 [D loss: 0.435983, acc: 78.12%] [G loss: 4.478144]\n",
      "497 [D loss: 0.417377, acc: 81.25%] [G loss: 4.017193]\n",
      "498 [D loss: 0.457110, acc: 84.38%] [G loss: 3.821479]\n",
      "499 [D loss: 0.467325, acc: 76.56%] [G loss: 4.499408]\n",
      "500 [D loss: 0.382775, acc: 81.25%] [G loss: 3.915806]\n",
      "501 [D loss: 0.378830, acc: 84.38%] [G loss: 4.717223]\n",
      "502 [D loss: 0.503262, acc: 67.19%] [G loss: 3.796550]\n",
      "503 [D loss: 0.452484, acc: 82.81%] [G loss: 4.352356]\n",
      "504 [D loss: 0.377859, acc: 78.12%] [G loss: 3.597067]\n",
      "505 [D loss: 0.537928, acc: 67.19%] [G loss: 4.149005]\n",
      "506 [D loss: 0.367510, acc: 79.69%] [G loss: 4.125952]\n",
      "507 [D loss: 0.255690, acc: 93.75%] [G loss: 3.909041]\n",
      "508 [D loss: 0.482833, acc: 75.00%] [G loss: 3.800925]\n",
      "509 [D loss: 0.410957, acc: 79.69%] [G loss: 4.449661]\n",
      "510 [D loss: 0.365995, acc: 84.38%] [G loss: 3.966661]\n",
      "511 [D loss: 0.209864, acc: 96.88%] [G loss: 4.270889]\n",
      "512 [D loss: 0.392792, acc: 85.94%] [G loss: 3.682736]\n",
      "513 [D loss: 0.412466, acc: 85.94%] [G loss: 4.777854]\n",
      "514 [D loss: 0.358968, acc: 85.94%] [G loss: 4.540504]\n",
      "515 [D loss: 0.524922, acc: 73.44%] [G loss: 3.624897]\n",
      "516 [D loss: 0.414501, acc: 84.38%] [G loss: 4.227514]\n",
      "517 [D loss: 0.368440, acc: 82.81%] [G loss: 4.045488]\n",
      "518 [D loss: 0.520886, acc: 73.44%] [G loss: 4.095598]\n",
      "519 [D loss: 0.383402, acc: 81.25%] [G loss: 4.366866]\n",
      "520 [D loss: 0.388389, acc: 89.06%] [G loss: 4.149144]\n",
      "521 [D loss: 0.370210, acc: 85.94%] [G loss: 4.633060]\n",
      "522 [D loss: 0.563204, acc: 70.31%] [G loss: 4.236460]\n",
      "523 [D loss: 0.465185, acc: 82.81%] [G loss: 4.134195]\n",
      "524 [D loss: 0.449926, acc: 79.69%] [G loss: 3.584003]\n",
      "525 [D loss: 0.348385, acc: 85.94%] [G loss: 3.867687]\n",
      "526 [D loss: 0.380128, acc: 78.12%] [G loss: 3.978649]\n",
      "527 [D loss: 0.350850, acc: 81.25%] [G loss: 3.820514]\n",
      "528 [D loss: 0.417118, acc: 78.12%] [G loss: 4.235571]\n",
      "529 [D loss: 0.384782, acc: 84.38%] [G loss: 4.098042]\n",
      "530 [D loss: 0.578868, acc: 64.06%] [G loss: 4.775503]\n",
      "531 [D loss: 0.321424, acc: 82.81%] [G loss: 3.735920]\n",
      "532 [D loss: 0.510420, acc: 76.56%] [G loss: 4.762836]\n",
      "533 [D loss: 0.398351, acc: 76.56%] [G loss: 3.779383]\n",
      "534 [D loss: 0.328132, acc: 89.06%] [G loss: 4.175767]\n",
      "535 [D loss: 0.515488, acc: 78.12%] [G loss: 4.438180]\n",
      "536 [D loss: 0.541314, acc: 70.31%] [G loss: 4.162484]\n",
      "537 [D loss: 0.411830, acc: 81.25%] [G loss: 3.883604]\n",
      "538 [D loss: 0.485380, acc: 73.44%] [G loss: 4.963387]\n",
      "539 [D loss: 0.336770, acc: 87.50%] [G loss: 3.971781]\n",
      "540 [D loss: 0.589284, acc: 68.75%] [G loss: 4.025878]\n",
      "541 [D loss: 0.276511, acc: 93.75%] [G loss: 3.777056]\n",
      "542 [D loss: 0.791637, acc: 56.25%] [G loss: 4.607019]\n",
      "543 [D loss: 0.313587, acc: 90.62%] [G loss: 4.178429]\n",
      "544 [D loss: 0.651756, acc: 71.88%] [G loss: 4.444313]\n",
      "545 [D loss: 0.407807, acc: 85.94%] [G loss: 3.690755]\n",
      "546 [D loss: 0.486946, acc: 75.00%] [G loss: 3.838080]\n",
      "547 [D loss: 0.407464, acc: 82.81%] [G loss: 4.294665]\n",
      "548 [D loss: 0.337452, acc: 89.06%] [G loss: 3.758551]\n",
      "549 [D loss: 0.428307, acc: 82.81%] [G loss: 3.869114]\n",
      "550 [D loss: 0.431590, acc: 84.38%] [G loss: 3.986718]\n",
      "551 [D loss: 0.312778, acc: 90.62%] [G loss: 3.873232]\n",
      "552 [D loss: 0.516024, acc: 71.88%] [G loss: 3.894033]\n",
      "553 [D loss: 0.352346, acc: 89.06%] [G loss: 4.293212]\n",
      "554 [D loss: 0.657387, acc: 68.75%] [G loss: 3.724630]\n",
      "555 [D loss: 0.475316, acc: 76.56%] [G loss: 3.824171]\n",
      "556 [D loss: 0.427631, acc: 79.69%] [G loss: 3.961602]\n",
      "557 [D loss: 0.385432, acc: 90.62%] [G loss: 3.907848]\n",
      "558 [D loss: 0.559082, acc: 75.00%] [G loss: 4.131065]\n",
      "559 [D loss: 0.423598, acc: 82.81%] [G loss: 3.672853]\n",
      "560 [D loss: 0.378315, acc: 85.94%] [G loss: 4.411116]\n",
      "561 [D loss: 0.391744, acc: 82.81%] [G loss: 4.050270]\n",
      "562 [D loss: 0.537398, acc: 70.31%] [G loss: 3.815919]\n",
      "563 [D loss: 0.594455, acc: 68.75%] [G loss: 3.352638]\n",
      "564 [D loss: 0.488823, acc: 75.00%] [G loss: 3.550289]\n",
      "565 [D loss: 0.355038, acc: 87.50%] [G loss: 3.764337]\n",
      "566 [D loss: 0.512032, acc: 75.00%] [G loss: 3.446485]\n",
      "567 [D loss: 0.559680, acc: 67.19%] [G loss: 3.922068]\n",
      "568 [D loss: 0.289509, acc: 92.19%] [G loss: 3.955865]\n",
      "569 [D loss: 0.719074, acc: 62.50%] [G loss: 4.508914]\n",
      "570 [D loss: 0.407379, acc: 79.69%] [G loss: 3.465926]\n",
      "571 [D loss: 0.479502, acc: 78.12%] [G loss: 4.268353]\n",
      "572 [D loss: 0.426869, acc: 79.69%] [G loss: 3.845002]\n",
      "573 [D loss: 0.368574, acc: 90.62%] [G loss: 4.626166]\n",
      "574 [D loss: 0.391524, acc: 82.81%] [G loss: 4.062379]\n",
      "575 [D loss: 0.586632, acc: 73.44%] [G loss: 3.795387]\n",
      "576 [D loss: 0.440778, acc: 82.81%] [G loss: 3.754486]\n",
      "577 [D loss: 0.499681, acc: 68.75%] [G loss: 4.353479]\n",
      "578 [D loss: 0.430283, acc: 79.69%] [G loss: 4.005842]\n",
      "579 [D loss: 0.411343, acc: 85.94%] [G loss: 3.948814]\n",
      "580 [D loss: 0.469303, acc: 81.25%] [G loss: 3.764315]\n",
      "581 [D loss: 0.477573, acc: 79.69%] [G loss: 3.804560]\n",
      "582 [D loss: 0.419822, acc: 85.94%] [G loss: 4.320639]\n",
      "583 [D loss: 0.468658, acc: 81.25%] [G loss: 3.636798]\n",
      "584 [D loss: 0.335963, acc: 87.50%] [G loss: 3.783204]\n",
      "585 [D loss: 0.531358, acc: 65.62%] [G loss: 3.969588]\n",
      "586 [D loss: 0.405904, acc: 84.38%] [G loss: 4.158333]\n",
      "587 [D loss: 0.363624, acc: 90.62%] [G loss: 3.566988]\n",
      "588 [D loss: 0.782336, acc: 51.56%] [G loss: 4.179636]\n",
      "589 [D loss: 0.502295, acc: 67.19%] [G loss: 3.979339]\n",
      "590 [D loss: 0.390164, acc: 84.38%] [G loss: 3.787120]\n",
      "591 [D loss: 0.453009, acc: 81.25%] [G loss: 3.860962]\n",
      "592 [D loss: 0.458688, acc: 73.44%] [G loss: 4.281344]\n",
      "593 [D loss: 0.538971, acc: 68.75%] [G loss: 4.071780]\n",
      "594 [D loss: 0.434325, acc: 78.12%] [G loss: 3.722287]\n",
      "595 [D loss: 0.487968, acc: 78.12%] [G loss: 3.910181]\n",
      "596 [D loss: 0.409190, acc: 81.25%] [G loss: 3.742586]\n",
      "597 [D loss: 0.361522, acc: 89.06%] [G loss: 4.102745]\n",
      "598 [D loss: 0.528194, acc: 68.75%] [G loss: 3.886543]\n",
      "599 [D loss: 0.690331, acc: 59.38%] [G loss: 3.741697]\n",
      "600 [D loss: 0.493626, acc: 76.56%] [G loss: 3.168506]\n",
      "601 [D loss: 0.300798, acc: 92.19%] [G loss: 3.854925]\n",
      "602 [D loss: 0.648384, acc: 67.19%] [G loss: 3.765177]\n",
      "603 [D loss: 0.475337, acc: 79.69%] [G loss: 3.593806]\n",
      "604 [D loss: 0.389678, acc: 85.94%] [G loss: 3.953061]\n",
      "605 [D loss: 0.431126, acc: 78.12%] [G loss: 3.704617]\n",
      "606 [D loss: 0.514616, acc: 76.56%] [G loss: 3.621769]\n",
      "607 [D loss: 0.624486, acc: 70.31%] [G loss: 3.609438]\n",
      "608 [D loss: 0.408595, acc: 85.94%] [G loss: 3.498076]\n",
      "609 [D loss: 0.623645, acc: 65.62%] [G loss: 4.064168]\n",
      "610 [D loss: 0.310467, acc: 89.06%] [G loss: 3.566619]\n",
      "611 [D loss: 0.517403, acc: 70.31%] [G loss: 4.057205]\n",
      "612 [D loss: 0.512582, acc: 76.56%] [G loss: 3.759822]\n",
      "613 [D loss: 0.555376, acc: 73.44%] [G loss: 3.108773]\n",
      "614 [D loss: 0.357506, acc: 85.94%] [G loss: 3.148048]\n",
      "615 [D loss: 0.609652, acc: 70.31%] [G loss: 3.505310]\n",
      "616 [D loss: 0.397331, acc: 87.50%] [G loss: 3.633864]\n",
      "617 [D loss: 0.491324, acc: 73.44%] [G loss: 3.684605]\n",
      "618 [D loss: 0.612555, acc: 64.06%] [G loss: 3.185629]\n",
      "619 [D loss: 0.481920, acc: 78.12%] [G loss: 3.567139]\n",
      "620 [D loss: 0.640104, acc: 62.50%] [G loss: 3.907313]\n",
      "621 [D loss: 0.689911, acc: 71.88%] [G loss: 3.321520]\n",
      "622 [D loss: 0.519514, acc: 68.75%] [G loss: 3.441886]\n",
      "623 [D loss: 0.645935, acc: 65.62%] [G loss: 3.293971]\n",
      "624 [D loss: 0.733561, acc: 50.00%] [G loss: 3.482373]\n",
      "625 [D loss: 0.404828, acc: 82.81%] [G loss: 3.797836]\n",
      "626 [D loss: 0.461786, acc: 75.00%] [G loss: 3.483094]\n",
      "627 [D loss: 0.610334, acc: 64.06%] [G loss: 3.557795]\n",
      "628 [D loss: 0.663756, acc: 68.75%] [G loss: 3.228802]\n",
      "629 [D loss: 0.432815, acc: 85.94%] [G loss: 3.654696]\n",
      "630 [D loss: 0.486785, acc: 81.25%] [G loss: 3.445814]\n",
      "631 [D loss: 0.462893, acc: 79.69%] [G loss: 3.533144]\n",
      "632 [D loss: 0.493524, acc: 75.00%] [G loss: 3.466701]\n",
      "633 [D loss: 0.616048, acc: 65.62%] [G loss: 3.421426]\n",
      "634 [D loss: 0.443385, acc: 84.38%] [G loss: 3.475590]\n",
      "635 [D loss: 0.450824, acc: 79.69%] [G loss: 3.059170]\n",
      "636 [D loss: 0.608703, acc: 57.81%] [G loss: 3.375829]\n",
      "637 [D loss: 0.556333, acc: 79.69%] [G loss: 3.003433]\n",
      "638 [D loss: 0.517710, acc: 76.56%] [G loss: 3.139503]\n",
      "639 [D loss: 0.400115, acc: 84.38%] [G loss: 3.319892]\n",
      "640 [D loss: 0.773342, acc: 53.12%] [G loss: 3.612938]\n",
      "641 [D loss: 0.488074, acc: 76.56%] [G loss: 3.183009]\n",
      "642 [D loss: 0.549666, acc: 73.44%] [G loss: 3.388823]\n",
      "643 [D loss: 0.605424, acc: 68.75%] [G loss: 3.385719]\n",
      "644 [D loss: 0.412676, acc: 79.69%] [G loss: 3.305111]\n",
      "645 [D loss: 0.572787, acc: 68.75%] [G loss: 3.078910]\n",
      "646 [D loss: 0.635490, acc: 65.62%] [G loss: 3.135030]\n",
      "647 [D loss: 0.471590, acc: 78.12%] [G loss: 3.224396]\n",
      "648 [D loss: 0.487249, acc: 73.44%] [G loss: 3.204248]\n",
      "649 [D loss: 0.525903, acc: 68.75%] [G loss: 2.888389]\n",
      "650 [D loss: 0.460586, acc: 78.12%] [G loss: 3.171640]\n",
      "651 [D loss: 0.516681, acc: 78.12%] [G loss: 3.441562]\n",
      "652 [D loss: 0.484368, acc: 82.81%] [G loss: 3.856929]\n",
      "653 [D loss: 0.469039, acc: 79.69%] [G loss: 3.070530]\n",
      "654 [D loss: 0.540973, acc: 70.31%] [G loss: 2.956789]\n",
      "655 [D loss: 0.505310, acc: 76.56%] [G loss: 3.468698]\n",
      "656 [D loss: 0.534516, acc: 73.44%] [G loss: 3.344951]\n",
      "657 [D loss: 0.782015, acc: 51.56%] [G loss: 3.232600]\n",
      "658 [D loss: 0.554204, acc: 82.81%] [G loss: 3.253415]\n",
      "659 [D loss: 0.477070, acc: 79.69%] [G loss: 3.096838]\n",
      "660 [D loss: 0.646812, acc: 64.06%] [G loss: 3.218920]\n",
      "661 [D loss: 0.492244, acc: 71.88%] [G loss: 3.029108]\n",
      "662 [D loss: 0.618772, acc: 65.62%] [G loss: 3.209536]\n",
      "663 [D loss: 0.545463, acc: 64.06%] [G loss: 2.987164]\n",
      "664 [D loss: 0.392806, acc: 79.69%] [G loss: 2.971460]\n",
      "665 [D loss: 0.577140, acc: 73.44%] [G loss: 3.294185]\n",
      "666 [D loss: 0.528812, acc: 75.00%] [G loss: 3.388536]\n",
      "667 [D loss: 0.625053, acc: 67.19%] [G loss: 3.070584]\n",
      "668 [D loss: 0.499188, acc: 76.56%] [G loss: 3.024280]\n",
      "669 [D loss: 0.588664, acc: 70.31%] [G loss: 3.212497]\n",
      "670 [D loss: 0.622008, acc: 64.06%] [G loss: 3.219226]\n",
      "671 [D loss: 0.595837, acc: 71.88%] [G loss: 3.507168]\n",
      "672 [D loss: 0.646653, acc: 67.19%] [G loss: 2.823145]\n",
      "673 [D loss: 0.502753, acc: 73.44%] [G loss: 2.991433]\n",
      "674 [D loss: 0.646459, acc: 64.06%] [G loss: 3.015184]\n",
      "675 [D loss: 0.492113, acc: 78.12%] [G loss: 3.305849]\n",
      "676 [D loss: 0.377816, acc: 84.38%] [G loss: 3.429816]\n",
      "677 [D loss: 0.636818, acc: 62.50%] [G loss: 3.023713]\n",
      "678 [D loss: 0.537711, acc: 75.00%] [G loss: 3.203984]\n",
      "679 [D loss: 0.368789, acc: 89.06%] [G loss: 2.996768]\n",
      "680 [D loss: 0.618017, acc: 68.75%] [G loss: 2.964760]\n",
      "681 [D loss: 0.588018, acc: 68.75%] [G loss: 2.897992]\n",
      "682 [D loss: 0.658968, acc: 56.25%] [G loss: 2.792537]\n",
      "683 [D loss: 0.448879, acc: 79.69%] [G loss: 2.964841]\n",
      "684 [D loss: 0.574771, acc: 73.44%] [G loss: 3.412406]\n",
      "685 [D loss: 0.576956, acc: 75.00%] [G loss: 2.786680]\n",
      "686 [D loss: 0.477747, acc: 79.69%] [G loss: 3.010923]\n",
      "687 [D loss: 0.480292, acc: 84.38%] [G loss: 2.987155]\n",
      "688 [D loss: 0.561661, acc: 71.88%] [G loss: 2.939364]\n",
      "689 [D loss: 0.548312, acc: 71.88%] [G loss: 3.129735]\n",
      "690 [D loss: 0.612616, acc: 68.75%] [G loss: 2.741846]\n",
      "691 [D loss: 0.498100, acc: 79.69%] [G loss: 3.096083]\n",
      "692 [D loss: 0.420132, acc: 85.94%] [G loss: 2.994265]\n",
      "693 [D loss: 0.661669, acc: 67.19%] [G loss: 3.245782]\n",
      "694 [D loss: 0.538773, acc: 73.44%] [G loss: 3.000124]\n",
      "695 [D loss: 0.483616, acc: 79.69%] [G loss: 2.862857]\n",
      "696 [D loss: 0.439875, acc: 81.25%] [G loss: 3.405387]\n",
      "697 [D loss: 0.616802, acc: 67.19%] [G loss: 2.652646]\n",
      "698 [D loss: 0.635648, acc: 70.31%] [G loss: 2.889704]\n",
      "699 [D loss: 0.637186, acc: 59.38%] [G loss: 2.601143]\n",
      "700 [D loss: 0.591723, acc: 62.50%] [G loss: 2.898315]\n",
      "701 [D loss: 0.611590, acc: 65.62%] [G loss: 2.957306]\n",
      "702 [D loss: 0.551471, acc: 71.88%] [G loss: 3.027159]\n",
      "703 [D loss: 0.468772, acc: 81.25%] [G loss: 2.728742]\n",
      "704 [D loss: 0.510306, acc: 79.69%] [G loss: 2.725988]\n",
      "705 [D loss: 0.674787, acc: 64.06%] [G loss: 2.651270]\n",
      "706 [D loss: 0.496127, acc: 76.56%] [G loss: 2.628670]\n",
      "707 [D loss: 0.611140, acc: 67.19%] [G loss: 3.026029]\n",
      "708 [D loss: 0.587944, acc: 65.62%] [G loss: 2.939897]\n",
      "709 [D loss: 0.518989, acc: 75.00%] [G loss: 3.165646]\n",
      "710 [D loss: 0.591635, acc: 67.19%] [G loss: 3.065813]\n",
      "711 [D loss: 0.593847, acc: 68.75%] [G loss: 2.575152]\n",
      "712 [D loss: 0.504049, acc: 78.12%] [G loss: 3.075205]\n",
      "713 [D loss: 0.496142, acc: 73.44%] [G loss: 2.752336]\n",
      "714 [D loss: 0.671350, acc: 62.50%] [G loss: 2.969207]\n",
      "715 [D loss: 0.515388, acc: 73.44%] [G loss: 2.826802]\n",
      "716 [D loss: 0.589811, acc: 65.62%] [G loss: 2.942334]\n",
      "717 [D loss: 0.554152, acc: 76.56%] [G loss: 2.907080]\n",
      "718 [D loss: 0.540648, acc: 75.00%] [G loss: 3.005479]\n",
      "719 [D loss: 0.534625, acc: 71.88%] [G loss: 2.989686]\n",
      "720 [D loss: 0.590185, acc: 60.94%] [G loss: 2.818290]\n",
      "721 [D loss: 0.604694, acc: 67.19%] [G loss: 2.998013]\n",
      "722 [D loss: 0.468735, acc: 84.38%] [G loss: 2.966067]\n",
      "723 [D loss: 0.665052, acc: 62.50%] [G loss: 2.734664]\n",
      "724 [D loss: 0.535733, acc: 71.88%] [G loss: 2.877960]\n",
      "725 [D loss: 0.468659, acc: 78.12%] [G loss: 3.017319]\n",
      "726 [D loss: 0.547125, acc: 76.56%] [G loss: 2.956681]\n",
      "727 [D loss: 0.560875, acc: 73.44%] [G loss: 2.944477]\n",
      "728 [D loss: 0.592629, acc: 68.75%] [G loss: 2.704455]\n",
      "729 [D loss: 0.488102, acc: 76.56%] [G loss: 2.728029]\n",
      "730 [D loss: 0.557348, acc: 64.06%] [G loss: 2.959693]\n",
      "731 [D loss: 0.497233, acc: 76.56%] [G loss: 2.929203]\n",
      "732 [D loss: 0.596237, acc: 70.31%] [G loss: 2.718482]\n",
      "733 [D loss: 0.509865, acc: 75.00%] [G loss: 3.225125]\n",
      "734 [D loss: 0.665897, acc: 60.94%] [G loss: 2.906210]\n",
      "735 [D loss: 0.590679, acc: 73.44%] [G loss: 3.008651]\n",
      "736 [D loss: 0.513535, acc: 70.31%] [G loss: 2.720283]\n",
      "737 [D loss: 0.508631, acc: 78.12%] [G loss: 2.922460]\n",
      "738 [D loss: 0.450895, acc: 87.50%] [G loss: 2.840128]\n",
      "739 [D loss: 0.495225, acc: 79.69%] [G loss: 2.920485]\n",
      "740 [D loss: 0.475879, acc: 79.69%] [G loss: 2.593355]\n",
      "741 [D loss: 0.519328, acc: 79.69%] [G loss: 3.035171]\n",
      "742 [D loss: 0.567521, acc: 68.75%] [G loss: 2.835532]\n",
      "743 [D loss: 0.536817, acc: 71.88%] [G loss: 3.497946]\n",
      "744 [D loss: 0.608896, acc: 67.19%] [G loss: 2.969240]\n",
      "745 [D loss: 0.644088, acc: 65.62%] [G loss: 2.596064]\n",
      "746 [D loss: 0.461624, acc: 82.81%] [G loss: 2.979585]\n",
      "747 [D loss: 0.500880, acc: 79.69%] [G loss: 3.002668]\n",
      "748 [D loss: 0.491222, acc: 75.00%] [G loss: 2.636659]\n",
      "749 [D loss: 0.557949, acc: 67.19%] [G loss: 2.925233]\n",
      "750 [D loss: 0.555834, acc: 73.44%] [G loss: 3.180676]\n",
      "751 [D loss: 0.489254, acc: 79.69%] [G loss: 3.077660]\n",
      "752 [D loss: 0.591230, acc: 62.50%] [G loss: 2.974608]\n",
      "753 [D loss: 0.440809, acc: 84.38%] [G loss: 2.650687]\n",
      "754 [D loss: 0.536328, acc: 71.88%] [G loss: 2.748833]\n",
      "755 [D loss: 0.422158, acc: 81.25%] [G loss: 2.891825]\n",
      "756 [D loss: 0.655637, acc: 62.50%] [G loss: 2.568249]\n",
      "757 [D loss: 0.426251, acc: 84.38%] [G loss: 2.996701]\n",
      "758 [D loss: 0.468911, acc: 75.00%] [G loss: 3.157328]\n",
      "759 [D loss: 0.579117, acc: 64.06%] [G loss: 3.031342]\n",
      "760 [D loss: 0.611005, acc: 71.88%] [G loss: 3.405171]\n",
      "761 [D loss: 0.591821, acc: 60.94%] [G loss: 2.644953]\n",
      "762 [D loss: 0.501130, acc: 81.25%] [G loss: 2.849617]\n",
      "763 [D loss: 0.678505, acc: 62.50%] [G loss: 2.808267]\n",
      "764 [D loss: 0.587638, acc: 67.19%] [G loss: 2.888024]\n",
      "765 [D loss: 0.504914, acc: 76.56%] [G loss: 2.762703]\n",
      "766 [D loss: 0.720156, acc: 51.56%] [G loss: 3.183169]\n",
      "767 [D loss: 0.624557, acc: 65.62%] [G loss: 2.714372]\n",
      "768 [D loss: 0.677729, acc: 64.06%] [G loss: 2.803182]\n",
      "769 [D loss: 0.422490, acc: 79.69%] [G loss: 2.719262]\n",
      "770 [D loss: 0.571929, acc: 64.06%] [G loss: 2.911111]\n",
      "771 [D loss: 0.484012, acc: 82.81%] [G loss: 3.207386]\n",
      "772 [D loss: 0.580315, acc: 71.88%] [G loss: 2.678917]\n",
      "773 [D loss: 0.500865, acc: 78.12%] [G loss: 3.199636]\n",
      "774 [D loss: 0.515710, acc: 71.88%] [G loss: 2.909083]\n",
      "775 [D loss: 0.489149, acc: 75.00%] [G loss: 2.991608]\n",
      "776 [D loss: 0.693323, acc: 62.50%] [G loss: 3.437267]\n",
      "777 [D loss: 0.576837, acc: 70.31%] [G loss: 3.174712]\n",
      "778 [D loss: 0.632727, acc: 64.06%] [G loss: 2.851277]\n",
      "779 [D loss: 0.564304, acc: 75.00%] [G loss: 2.954847]\n",
      "780 [D loss: 0.576807, acc: 68.75%] [G loss: 3.194022]\n",
      "781 [D loss: 0.570723, acc: 70.31%] [G loss: 2.882566]\n",
      "782 [D loss: 0.549033, acc: 73.44%] [G loss: 2.782213]\n",
      "783 [D loss: 0.646387, acc: 62.50%] [G loss: 2.880896]\n",
      "784 [D loss: 0.564285, acc: 76.56%] [G loss: 2.837606]\n",
      "785 [D loss: 0.518664, acc: 75.00%] [G loss: 2.803221]\n",
      "786 [D loss: 0.636665, acc: 68.75%] [G loss: 2.897265]\n",
      "787 [D loss: 0.548185, acc: 71.88%] [G loss: 2.903476]\n",
      "788 [D loss: 0.590916, acc: 67.19%] [G loss: 2.810436]\n",
      "789 [D loss: 0.552484, acc: 71.88%] [G loss: 2.734848]\n",
      "790 [D loss: 0.447600, acc: 84.38%] [G loss: 3.265520]\n",
      "791 [D loss: 0.700163, acc: 57.81%] [G loss: 2.679112]\n",
      "792 [D loss: 0.526988, acc: 73.44%] [G loss: 3.231899]\n",
      "793 [D loss: 0.550558, acc: 68.75%] [G loss: 2.702257]\n",
      "794 [D loss: 0.643046, acc: 62.50%] [G loss: 2.704827]\n",
      "795 [D loss: 0.493434, acc: 76.56%] [G loss: 2.912981]\n",
      "796 [D loss: 0.523216, acc: 78.12%] [G loss: 2.634404]\n",
      "797 [D loss: 0.478340, acc: 71.88%] [G loss: 2.919306]\n",
      "798 [D loss: 0.591638, acc: 68.75%] [G loss: 2.740087]\n",
      "799 [D loss: 0.596251, acc: 75.00%] [G loss: 3.102796]\n",
      "800 [D loss: 0.571392, acc: 67.19%] [G loss: 2.930544]\n",
      "801 [D loss: 0.599107, acc: 70.31%] [G loss: 2.981252]\n",
      "802 [D loss: 0.614387, acc: 67.19%] [G loss: 3.007125]\n",
      "803 [D loss: 0.659199, acc: 62.50%] [G loss: 2.800969]\n",
      "804 [D loss: 0.648552, acc: 59.38%] [G loss: 3.055097]\n",
      "805 [D loss: 0.477484, acc: 78.12%] [G loss: 2.892245]\n",
      "806 [D loss: 0.541175, acc: 75.00%] [G loss: 2.859212]\n",
      "807 [D loss: 0.608866, acc: 71.88%] [G loss: 2.684917]\n",
      "808 [D loss: 0.463820, acc: 76.56%] [G loss: 3.177996]\n",
      "809 [D loss: 0.568046, acc: 70.31%] [G loss: 3.096779]\n",
      "810 [D loss: 0.623083, acc: 62.50%] [G loss: 2.693038]\n",
      "811 [D loss: 0.588467, acc: 68.75%] [G loss: 2.583665]\n",
      "812 [D loss: 0.646857, acc: 60.94%] [G loss: 3.017294]\n",
      "813 [D loss: 0.558737, acc: 75.00%] [G loss: 2.926753]\n",
      "814 [D loss: 0.538233, acc: 71.88%] [G loss: 2.855764]\n",
      "815 [D loss: 0.537571, acc: 73.44%] [G loss: 2.915790]\n",
      "816 [D loss: 0.565664, acc: 68.75%] [G loss: 2.716539]\n",
      "817 [D loss: 0.509891, acc: 71.88%] [G loss: 2.616505]\n",
      "818 [D loss: 0.578140, acc: 68.75%] [G loss: 3.054764]\n",
      "819 [D loss: 0.518301, acc: 76.56%] [G loss: 2.984004]\n",
      "820 [D loss: 0.488018, acc: 75.00%] [G loss: 3.096389]\n",
      "821 [D loss: 0.601388, acc: 67.19%] [G loss: 2.816274]\n",
      "822 [D loss: 0.616809, acc: 64.06%] [G loss: 2.694604]\n",
      "823 [D loss: 0.696446, acc: 62.50%] [G loss: 2.858550]\n",
      "824 [D loss: 0.610624, acc: 65.62%] [G loss: 2.694972]\n",
      "825 [D loss: 0.529175, acc: 75.00%] [G loss: 2.955440]\n",
      "826 [D loss: 0.576982, acc: 67.19%] [G loss: 2.744962]\n",
      "827 [D loss: 0.493690, acc: 79.69%] [G loss: 2.949033]\n",
      "828 [D loss: 0.497800, acc: 78.12%] [G loss: 3.243436]\n",
      "829 [D loss: 0.560170, acc: 64.06%] [G loss: 2.868040]\n",
      "830 [D loss: 0.508426, acc: 76.56%] [G loss: 2.866947]\n",
      "831 [D loss: 0.578589, acc: 67.19%] [G loss: 2.687156]\n",
      "832 [D loss: 0.665729, acc: 64.06%] [G loss: 2.718075]\n",
      "833 [D loss: 0.575789, acc: 75.00%] [G loss: 2.900570]\n",
      "834 [D loss: 0.469098, acc: 78.12%] [G loss: 3.004663]\n",
      "835 [D loss: 0.512018, acc: 78.12%] [G loss: 3.105484]\n",
      "836 [D loss: 0.436443, acc: 81.25%] [G loss: 3.047436]\n",
      "837 [D loss: 0.488779, acc: 79.69%] [G loss: 2.727322]\n",
      "838 [D loss: 0.525591, acc: 79.69%] [G loss: 3.172921]\n",
      "839 [D loss: 0.470494, acc: 73.44%] [G loss: 2.823529]\n",
      "840 [D loss: 0.535934, acc: 73.44%] [G loss: 3.196326]\n",
      "841 [D loss: 0.541479, acc: 75.00%] [G loss: 3.038102]\n",
      "842 [D loss: 0.564822, acc: 70.31%] [G loss: 2.920750]\n",
      "843 [D loss: 0.573174, acc: 65.62%] [G loss: 2.857541]\n",
      "844 [D loss: 0.670562, acc: 57.81%] [G loss: 2.618985]\n",
      "845 [D loss: 0.462158, acc: 81.25%] [G loss: 2.914868]\n",
      "846 [D loss: 0.645634, acc: 67.19%] [G loss: 3.297326]\n",
      "847 [D loss: 0.569412, acc: 64.06%] [G loss: 2.618372]\n",
      "848 [D loss: 0.446674, acc: 82.81%] [G loss: 3.185930]\n",
      "849 [D loss: 0.547466, acc: 76.56%] [G loss: 3.159912]\n",
      "850 [D loss: 0.460744, acc: 79.69%] [G loss: 3.119119]\n",
      "851 [D loss: 0.542403, acc: 67.19%] [G loss: 2.938397]\n",
      "852 [D loss: 0.581681, acc: 65.62%] [G loss: 2.944436]\n",
      "853 [D loss: 0.499355, acc: 75.00%] [G loss: 2.903980]\n",
      "854 [D loss: 0.580745, acc: 68.75%] [G loss: 2.916101]\n",
      "855 [D loss: 0.608526, acc: 65.62%] [G loss: 2.519438]\n",
      "856 [D loss: 0.533387, acc: 75.00%] [G loss: 3.264684]\n",
      "857 [D loss: 0.607975, acc: 68.75%] [G loss: 2.893686]\n",
      "858 [D loss: 0.492956, acc: 71.88%] [G loss: 3.231506]\n",
      "859 [D loss: 0.524914, acc: 76.56%] [G loss: 2.960373]\n",
      "860 [D loss: 0.539173, acc: 75.00%] [G loss: 3.002440]\n",
      "861 [D loss: 0.549353, acc: 75.00%] [G loss: 3.239605]\n",
      "862 [D loss: 0.617819, acc: 71.88%] [G loss: 3.102979]\n",
      "863 [D loss: 0.462214, acc: 81.25%] [G loss: 2.932452]\n",
      "864 [D loss: 0.494268, acc: 70.31%] [G loss: 3.475545]\n",
      "865 [D loss: 0.566868, acc: 73.44%] [G loss: 3.544683]\n",
      "866 [D loss: 0.574831, acc: 68.75%] [G loss: 2.953446]\n",
      "867 [D loss: 0.560773, acc: 79.69%] [G loss: 3.460513]\n",
      "868 [D loss: 0.600189, acc: 65.62%] [G loss: 3.253448]\n",
      "869 [D loss: 0.675801, acc: 60.94%] [G loss: 3.080135]\n",
      "870 [D loss: 0.528681, acc: 71.88%] [G loss: 3.321107]\n",
      "871 [D loss: 0.588549, acc: 71.88%] [G loss: 3.137343]\n",
      "872 [D loss: 0.551645, acc: 64.06%] [G loss: 2.907506]\n",
      "873 [D loss: 0.377471, acc: 89.06%] [G loss: 3.044278]\n",
      "874 [D loss: 0.640420, acc: 60.94%] [G loss: 2.730593]\n",
      "875 [D loss: 0.398316, acc: 85.94%] [G loss: 3.313923]\n",
      "876 [D loss: 0.434191, acc: 79.69%] [G loss: 3.219994]\n",
      "877 [D loss: 0.378911, acc: 85.94%] [G loss: 3.029689]\n",
      "878 [D loss: 0.487249, acc: 78.12%] [G loss: 3.299356]\n",
      "879 [D loss: 0.672950, acc: 57.81%] [G loss: 2.729415]\n",
      "880 [D loss: 0.415460, acc: 81.25%] [G loss: 2.787808]\n",
      "881 [D loss: 0.698248, acc: 62.50%] [G loss: 3.109938]\n",
      "882 [D loss: 0.609299, acc: 73.44%] [G loss: 3.001848]\n",
      "883 [D loss: 0.506529, acc: 73.44%] [G loss: 3.138639]\n",
      "884 [D loss: 0.454201, acc: 78.12%] [G loss: 2.973707]\n",
      "885 [D loss: 0.623080, acc: 68.75%] [G loss: 3.077204]\n",
      "886 [D loss: 0.609577, acc: 60.94%] [G loss: 3.320542]\n",
      "887 [D loss: 0.669473, acc: 67.19%] [G loss: 2.705474]\n",
      "888 [D loss: 0.504594, acc: 73.44%] [G loss: 3.725398]\n",
      "889 [D loss: 0.503329, acc: 75.00%] [G loss: 3.329737]\n",
      "890 [D loss: 0.574560, acc: 75.00%] [G loss: 3.028265]\n",
      "891 [D loss: 0.545159, acc: 73.44%] [G loss: 3.262998]\n",
      "892 [D loss: 0.549686, acc: 71.88%] [G loss: 3.002287]\n",
      "893 [D loss: 0.665976, acc: 65.62%] [G loss: 3.147072]\n",
      "894 [D loss: 0.605827, acc: 68.75%] [G loss: 2.668384]\n",
      "895 [D loss: 0.431734, acc: 84.38%] [G loss: 3.178649]\n",
      "896 [D loss: 0.491918, acc: 71.88%] [G loss: 2.816244]\n",
      "897 [D loss: 0.605112, acc: 68.75%] [G loss: 2.808477]\n",
      "898 [D loss: 0.514703, acc: 73.44%] [G loss: 3.443457]\n",
      "899 [D loss: 0.500474, acc: 81.25%] [G loss: 3.109636]\n",
      "900 [D loss: 0.529588, acc: 75.00%] [G loss: 2.499957]\n",
      "901 [D loss: 0.515744, acc: 73.44%] [G loss: 3.418213]\n",
      "902 [D loss: 0.515700, acc: 70.31%] [G loss: 2.966941]\n",
      "903 [D loss: 0.556129, acc: 75.00%] [G loss: 2.843276]\n",
      "904 [D loss: 0.602336, acc: 68.75%] [G loss: 3.028783]\n",
      "905 [D loss: 0.549839, acc: 76.56%] [G loss: 2.928142]\n",
      "906 [D loss: 0.632931, acc: 60.94%] [G loss: 2.886432]\n",
      "907 [D loss: 0.590538, acc: 70.31%] [G loss: 2.857312]\n",
      "908 [D loss: 0.504207, acc: 78.12%] [G loss: 3.087888]\n",
      "909 [D loss: 0.567141, acc: 70.31%] [G loss: 2.873168]\n",
      "910 [D loss: 0.521798, acc: 76.56%] [G loss: 3.006412]\n",
      "911 [D loss: 0.444250, acc: 76.56%] [G loss: 2.734159]\n",
      "912 [D loss: 0.503838, acc: 75.00%] [G loss: 2.759246]\n",
      "913 [D loss: 0.651595, acc: 68.75%] [G loss: 3.354495]\n",
      "914 [D loss: 0.448094, acc: 81.25%] [G loss: 3.114240]\n",
      "915 [D loss: 0.572888, acc: 73.44%] [G loss: 3.054929]\n",
      "916 [D loss: 0.664236, acc: 57.81%] [G loss: 2.938305]\n",
      "917 [D loss: 0.530694, acc: 73.44%] [G loss: 2.707924]\n",
      "918 [D loss: 0.661256, acc: 67.19%] [G loss: 2.822701]\n",
      "919 [D loss: 0.629228, acc: 62.50%] [G loss: 3.285399]\n",
      "920 [D loss: 0.524003, acc: 71.88%] [G loss: 3.065571]\n",
      "921 [D loss: 0.504682, acc: 71.88%] [G loss: 2.576048]\n",
      "922 [D loss: 0.583950, acc: 70.31%] [G loss: 2.913955]\n",
      "923 [D loss: 0.666629, acc: 64.06%] [G loss: 2.686497]\n",
      "924 [D loss: 0.545609, acc: 68.75%] [G loss: 3.266328]\n",
      "925 [D loss: 0.538647, acc: 70.31%] [G loss: 3.241355]\n",
      "926 [D loss: 0.517687, acc: 75.00%] [G loss: 3.550827]\n",
      "927 [D loss: 0.642144, acc: 57.81%] [G loss: 2.662605]\n",
      "928 [D loss: 0.474838, acc: 79.69%] [G loss: 2.845260]\n",
      "929 [D loss: 0.645614, acc: 59.38%] [G loss: 3.108697]\n",
      "930 [D loss: 0.446255, acc: 76.56%] [G loss: 3.229458]\n",
      "931 [D loss: 0.796643, acc: 59.38%] [G loss: 3.312299]\n",
      "932 [D loss: 0.607337, acc: 67.19%] [G loss: 3.222186]\n",
      "933 [D loss: 0.658829, acc: 59.38%] [G loss: 2.634084]\n",
      "934 [D loss: 0.603850, acc: 67.19%] [G loss: 2.992438]\n",
      "935 [D loss: 0.529938, acc: 70.31%] [G loss: 3.023919]\n",
      "936 [D loss: 0.528543, acc: 73.44%] [G loss: 2.949983]\n",
      "937 [D loss: 0.486610, acc: 81.25%] [G loss: 3.032163]\n",
      "938 [D loss: 0.601342, acc: 68.75%] [G loss: 3.031216]\n",
      "939 [D loss: 0.522203, acc: 76.56%] [G loss: 2.573186]\n",
      "940 [D loss: 0.685372, acc: 59.38%] [G loss: 2.766021]\n",
      "941 [D loss: 0.484525, acc: 76.56%] [G loss: 2.988055]\n",
      "942 [D loss: 0.554658, acc: 70.31%] [G loss: 2.924183]\n",
      "943 [D loss: 0.567368, acc: 70.31%] [G loss: 2.931552]\n",
      "944 [D loss: 0.627905, acc: 60.94%] [G loss: 2.742851]\n",
      "945 [D loss: 0.536906, acc: 71.88%] [G loss: 3.283963]\n",
      "946 [D loss: 0.629276, acc: 65.62%] [G loss: 3.317359]\n",
      "947 [D loss: 0.606469, acc: 65.62%] [G loss: 3.228459]\n",
      "948 [D loss: 0.565438, acc: 67.19%] [G loss: 3.278350]\n",
      "949 [D loss: 0.488961, acc: 78.12%] [G loss: 3.311180]\n",
      "950 [D loss: 0.596859, acc: 60.94%] [G loss: 2.940581]\n",
      "951 [D loss: 0.614740, acc: 71.88%] [G loss: 2.676810]\n",
      "952 [D loss: 0.614425, acc: 70.31%] [G loss: 2.671140]\n",
      "953 [D loss: 0.594806, acc: 64.06%] [G loss: 2.800633]\n",
      "954 [D loss: 0.540029, acc: 68.75%] [G loss: 2.893135]\n",
      "955 [D loss: 0.531614, acc: 67.19%] [G loss: 3.148579]\n",
      "956 [D loss: 0.497547, acc: 78.12%] [G loss: 2.863567]\n",
      "957 [D loss: 0.512472, acc: 76.56%] [G loss: 3.109815]\n",
      "958 [D loss: 0.455074, acc: 76.56%] [G loss: 3.053580]\n",
      "959 [D loss: 0.723180, acc: 65.62%] [G loss: 2.933864]\n",
      "960 [D loss: 0.601894, acc: 64.06%] [G loss: 2.896275]\n",
      "961 [D loss: 0.536496, acc: 73.44%] [G loss: 2.887465]\n",
      "962 [D loss: 0.571092, acc: 70.31%] [G loss: 3.088860]\n",
      "963 [D loss: 0.487856, acc: 78.12%] [G loss: 3.043435]\n",
      "964 [D loss: 0.646823, acc: 64.06%] [G loss: 3.127798]\n",
      "965 [D loss: 0.406733, acc: 87.50%] [G loss: 2.780648]\n",
      "966 [D loss: 0.612756, acc: 59.38%] [G loss: 2.978307]\n",
      "967 [D loss: 0.634253, acc: 64.06%] [G loss: 2.972584]\n",
      "968 [D loss: 0.682711, acc: 59.38%] [G loss: 2.778760]\n",
      "969 [D loss: 0.560156, acc: 71.88%] [G loss: 2.959376]\n",
      "970 [D loss: 0.585985, acc: 64.06%] [G loss: 2.877920]\n",
      "971 [D loss: 0.523772, acc: 75.00%] [G loss: 2.435102]\n",
      "972 [D loss: 0.441688, acc: 78.12%] [G loss: 2.879042]\n",
      "973 [D loss: 0.627850, acc: 67.19%] [G loss: 2.923126]\n",
      "974 [D loss: 0.573785, acc: 73.44%] [G loss: 2.981505]\n",
      "975 [D loss: 0.637890, acc: 64.06%] [G loss: 2.523665]\n",
      "976 [D loss: 0.410330, acc: 79.69%] [G loss: 2.854968]\n",
      "977 [D loss: 0.526418, acc: 73.44%] [G loss: 3.065236]\n",
      "978 [D loss: 0.540580, acc: 75.00%] [G loss: 2.984560]\n",
      "979 [D loss: 0.507620, acc: 76.56%] [G loss: 3.195549]\n",
      "980 [D loss: 0.688538, acc: 57.81%] [G loss: 2.722167]\n",
      "981 [D loss: 0.635597, acc: 67.19%] [G loss: 2.931370]\n",
      "982 [D loss: 0.632654, acc: 59.38%] [G loss: 2.997316]\n",
      "983 [D loss: 0.633651, acc: 68.75%] [G loss: 2.727975]\n",
      "984 [D loss: 0.524150, acc: 70.31%] [G loss: 2.564861]\n",
      "985 [D loss: 0.527368, acc: 78.12%] [G loss: 2.863284]\n",
      "986 [D loss: 0.572529, acc: 65.62%] [G loss: 3.031711]\n",
      "987 [D loss: 0.549286, acc: 67.19%] [G loss: 2.934865]\n",
      "988 [D loss: 0.866770, acc: 43.75%] [G loss: 2.658226]\n",
      "989 [D loss: 0.553680, acc: 65.62%] [G loss: 2.859712]\n",
      "990 [D loss: 0.638905, acc: 62.50%] [G loss: 3.275504]\n",
      "991 [D loss: 0.658856, acc: 62.50%] [G loss: 2.941733]\n",
      "992 [D loss: 0.665949, acc: 67.19%] [G loss: 2.697284]\n",
      "993 [D loss: 0.497036, acc: 79.69%] [G loss: 2.744187]\n",
      "994 [D loss: 0.572718, acc: 71.88%] [G loss: 2.590084]\n",
      "995 [D loss: 0.504292, acc: 76.56%] [G loss: 2.988409]\n",
      "996 [D loss: 0.541425, acc: 71.88%] [G loss: 2.866582]\n",
      "997 [D loss: 0.571832, acc: 67.19%] [G loss: 2.861608]\n",
      "998 [D loss: 0.501511, acc: 79.69%] [G loss: 2.772533]\n",
      "999 [D loss: 0.581370, acc: 76.56%] [G loss: 2.933622]\n",
      "1000 [D loss: 0.583941, acc: 62.50%] [G loss: 2.879050]\n",
      "1001 [D loss: 0.562310, acc: 73.44%] [G loss: 2.781693]\n",
      "1002 [D loss: 0.651047, acc: 67.19%] [G loss: 2.717246]\n",
      "1003 [D loss: 0.529115, acc: 65.62%] [G loss: 2.903977]\n",
      "1004 [D loss: 0.671577, acc: 56.25%] [G loss: 2.864267]\n",
      "1005 [D loss: 0.602470, acc: 68.75%] [G loss: 2.413015]\n",
      "1006 [D loss: 0.701018, acc: 59.38%] [G loss: 2.655267]\n",
      "1007 [D loss: 0.514453, acc: 70.31%] [G loss: 2.642890]\n",
      "1008 [D loss: 0.537785, acc: 78.12%] [G loss: 2.559394]\n",
      "1009 [D loss: 0.575340, acc: 70.31%] [G loss: 3.059307]\n",
      "1010 [D loss: 0.621595, acc: 60.94%] [G loss: 2.554295]\n",
      "1011 [D loss: 0.546655, acc: 76.56%] [G loss: 2.882810]\n",
      "1012 [D loss: 0.698200, acc: 59.38%] [G loss: 2.754034]\n",
      "1013 [D loss: 0.588612, acc: 64.06%] [G loss: 2.763209]\n",
      "1014 [D loss: 0.600097, acc: 57.81%] [G loss: 2.548203]\n",
      "1015 [D loss: 0.629107, acc: 60.94%] [G loss: 2.625461]\n",
      "1016 [D loss: 0.584486, acc: 64.06%] [G loss: 2.976316]\n",
      "1017 [D loss: 0.570150, acc: 73.44%] [G loss: 3.057209]\n",
      "1018 [D loss: 0.531350, acc: 73.44%] [G loss: 2.840547]\n",
      "1019 [D loss: 0.642030, acc: 62.50%] [G loss: 2.984266]\n",
      "1020 [D loss: 0.544168, acc: 75.00%] [G loss: 2.793330]\n",
      "1021 [D loss: 0.599804, acc: 67.19%] [G loss: 2.750165]\n",
      "1022 [D loss: 0.636769, acc: 60.94%] [G loss: 2.515795]\n",
      "1023 [D loss: 0.495872, acc: 81.25%] [G loss: 3.079618]\n",
      "1024 [D loss: 0.491325, acc: 76.56%] [G loss: 2.760871]\n",
      "1025 [D loss: 0.611788, acc: 68.75%] [G loss: 2.515297]\n",
      "1026 [D loss: 0.469602, acc: 85.94%] [G loss: 2.426966]\n",
      "1027 [D loss: 0.577506, acc: 62.50%] [G loss: 2.656140]\n",
      "1028 [D loss: 0.426331, acc: 82.81%] [G loss: 2.900555]\n",
      "1029 [D loss: 0.542577, acc: 73.44%] [G loss: 3.262705]\n",
      "1030 [D loss: 0.847965, acc: 43.75%] [G loss: 2.590421]\n",
      "1031 [D loss: 0.664550, acc: 62.50%] [G loss: 2.971142]\n",
      "1032 [D loss: 0.540597, acc: 73.44%] [G loss: 2.496125]\n",
      "1033 [D loss: 0.680449, acc: 65.62%] [G loss: 2.750761]\n",
      "1034 [D loss: 0.580585, acc: 64.06%] [G loss: 2.832156]\n",
      "1035 [D loss: 0.638983, acc: 64.06%] [G loss: 2.818621]\n",
      "1036 [D loss: 0.615413, acc: 64.06%] [G loss: 2.787054]\n",
      "1037 [D loss: 0.647396, acc: 67.19%] [G loss: 2.537676]\n",
      "1038 [D loss: 0.549652, acc: 71.88%] [G loss: 2.861761]\n",
      "1039 [D loss: 0.572287, acc: 68.75%] [G loss: 2.916011]\n",
      "1040 [D loss: 0.742369, acc: 54.69%] [G loss: 2.558567]\n",
      "1041 [D loss: 0.584068, acc: 70.31%] [G loss: 2.513812]\n",
      "1042 [D loss: 0.487026, acc: 73.44%] [G loss: 2.809637]\n",
      "1043 [D loss: 0.566709, acc: 60.94%] [G loss: 2.824259]\n",
      "1044 [D loss: 0.618912, acc: 65.62%] [G loss: 2.685970]\n",
      "1045 [D loss: 0.626590, acc: 62.50%] [G loss: 2.727545]\n",
      "1046 [D loss: 0.578877, acc: 62.50%] [G loss: 2.434536]\n",
      "1047 [D loss: 0.670528, acc: 62.50%] [G loss: 2.795218]\n",
      "1048 [D loss: 0.643903, acc: 67.19%] [G loss: 2.766158]\n",
      "1049 [D loss: 0.607493, acc: 65.62%] [G loss: 2.314726]\n",
      "1050 [D loss: 0.616404, acc: 64.06%] [G loss: 2.933308]\n",
      "1051 [D loss: 0.511419, acc: 73.44%] [G loss: 3.015728]\n",
      "1052 [D loss: 0.495057, acc: 76.56%] [G loss: 2.829927]\n",
      "1053 [D loss: 0.566031, acc: 67.19%] [G loss: 2.757017]\n",
      "1054 [D loss: 0.530250, acc: 68.75%] [G loss: 2.817704]\n",
      "1055 [D loss: 0.600458, acc: 60.94%] [G loss: 2.408537]\n",
      "1056 [D loss: 0.706967, acc: 62.50%] [G loss: 2.449208]\n",
      "1057 [D loss: 0.536740, acc: 75.00%] [G loss: 2.909461]\n",
      "1058 [D loss: 0.580519, acc: 71.88%] [G loss: 2.491647]\n",
      "1059 [D loss: 0.611895, acc: 70.31%] [G loss: 2.725780]\n",
      "1060 [D loss: 0.479810, acc: 75.00%] [G loss: 2.825265]\n",
      "1061 [D loss: 0.563953, acc: 75.00%] [G loss: 2.564185]\n",
      "1062 [D loss: 0.677941, acc: 59.38%] [G loss: 2.741681]\n",
      "1063 [D loss: 0.575803, acc: 70.31%] [G loss: 2.563084]\n",
      "1064 [D loss: 0.582360, acc: 73.44%] [G loss: 2.561011]\n",
      "1065 [D loss: 0.624256, acc: 62.50%] [G loss: 2.958282]\n",
      "1066 [D loss: 0.551787, acc: 71.88%] [G loss: 2.758730]\n",
      "1067 [D loss: 0.639262, acc: 64.06%] [G loss: 2.516999]\n",
      "1068 [D loss: 0.489359, acc: 70.31%] [G loss: 2.718108]\n",
      "1069 [D loss: 0.575483, acc: 71.88%] [G loss: 2.597973]\n",
      "1070 [D loss: 0.536019, acc: 73.44%] [G loss: 2.691622]\n",
      "1071 [D loss: 0.676059, acc: 59.38%] [G loss: 2.688841]\n",
      "1072 [D loss: 0.519329, acc: 75.00%] [G loss: 2.646006]\n",
      "1073 [D loss: 0.582186, acc: 64.06%] [G loss: 2.933184]\n",
      "1074 [D loss: 0.509277, acc: 78.12%] [G loss: 2.573123]\n",
      "1075 [D loss: 0.535207, acc: 71.88%] [G loss: 2.350430]\n",
      "1076 [D loss: 0.613338, acc: 65.62%] [G loss: 2.940067]\n",
      "1077 [D loss: 0.634674, acc: 64.06%] [G loss: 2.569063]\n",
      "1078 [D loss: 0.710713, acc: 60.94%] [G loss: 2.386858]\n",
      "1079 [D loss: 0.652703, acc: 65.62%] [G loss: 2.498299]\n",
      "1080 [D loss: 0.638799, acc: 59.38%] [G loss: 2.976070]\n",
      "1081 [D loss: 0.624160, acc: 65.62%] [G loss: 2.811096]\n",
      "1082 [D loss: 0.565457, acc: 64.06%] [G loss: 2.690277]\n",
      "1083 [D loss: 0.565521, acc: 67.19%] [G loss: 2.622468]\n",
      "1084 [D loss: 0.507571, acc: 76.56%] [G loss: 2.698359]\n",
      "1085 [D loss: 0.562020, acc: 73.44%] [G loss: 2.830847]\n",
      "1086 [D loss: 0.545338, acc: 68.75%] [G loss: 2.918440]\n",
      "1087 [D loss: 0.671191, acc: 60.94%] [G loss: 2.770434]\n",
      "1088 [D loss: 0.544274, acc: 73.44%] [G loss: 2.291970]\n",
      "1089 [D loss: 0.479267, acc: 78.12%] [G loss: 2.761361]\n",
      "1090 [D loss: 0.636172, acc: 68.75%] [G loss: 2.829949]\n",
      "1091 [D loss: 0.787393, acc: 62.50%] [G loss: 2.550744]\n",
      "1092 [D loss: 0.568953, acc: 70.31%] [G loss: 2.394866]\n",
      "1093 [D loss: 0.543672, acc: 68.75%] [G loss: 2.849423]\n",
      "1094 [D loss: 0.505679, acc: 76.56%] [G loss: 2.607987]\n",
      "1095 [D loss: 0.775994, acc: 54.69%] [G loss: 2.517341]\n",
      "1096 [D loss: 0.547123, acc: 73.44%] [G loss: 2.525153]\n",
      "1097 [D loss: 0.721176, acc: 59.38%] [G loss: 2.476892]\n",
      "1098 [D loss: 0.493556, acc: 82.81%] [G loss: 2.704271]\n",
      "1099 [D loss: 0.581759, acc: 76.56%] [G loss: 2.983185]\n",
      "1100 [D loss: 0.596338, acc: 65.62%] [G loss: 2.841491]\n",
      "1101 [D loss: 0.729941, acc: 57.81%] [G loss: 2.335950]\n",
      "1102 [D loss: 0.583179, acc: 70.31%] [G loss: 2.520549]\n",
      "1103 [D loss: 0.624495, acc: 57.81%] [G loss: 2.457959]\n",
      "1104 [D loss: 0.570734, acc: 75.00%] [G loss: 2.805426]\n",
      "1105 [D loss: 0.657100, acc: 64.06%] [G loss: 2.990227]\n",
      "1106 [D loss: 0.623064, acc: 65.62%] [G loss: 2.532946]\n",
      "1107 [D loss: 0.512546, acc: 78.12%] [G loss: 2.471142]\n",
      "1108 [D loss: 0.583207, acc: 68.75%] [G loss: 2.585582]\n",
      "1109 [D loss: 0.837610, acc: 45.31%] [G loss: 2.356786]\n",
      "1110 [D loss: 0.596515, acc: 65.62%] [G loss: 2.919022]\n",
      "1111 [D loss: 0.634095, acc: 62.50%] [G loss: 2.542887]\n",
      "1112 [D loss: 0.566520, acc: 71.88%] [G loss: 2.568860]\n",
      "1113 [D loss: 0.567942, acc: 73.44%] [G loss: 2.677794]\n",
      "1114 [D loss: 0.553178, acc: 76.56%] [G loss: 2.713135]\n",
      "1115 [D loss: 0.632667, acc: 71.88%] [G loss: 2.956701]\n",
      "1116 [D loss: 0.466906, acc: 82.81%] [G loss: 2.491297]\n",
      "1117 [D loss: 0.499751, acc: 76.56%] [G loss: 2.600997]\n",
      "1118 [D loss: 0.570771, acc: 70.31%] [G loss: 2.518706]\n",
      "1119 [D loss: 0.551442, acc: 71.88%] [G loss: 2.817152]\n",
      "1120 [D loss: 0.499218, acc: 78.12%] [G loss: 3.263897]\n",
      "1121 [D loss: 0.548777, acc: 70.31%] [G loss: 2.538496]\n",
      "1122 [D loss: 0.720301, acc: 54.69%] [G loss: 2.555111]\n",
      "1123 [D loss: 0.480222, acc: 81.25%] [G loss: 2.857243]\n",
      "1124 [D loss: 0.503537, acc: 78.12%] [G loss: 2.523641]\n",
      "1125 [D loss: 0.537818, acc: 65.62%] [G loss: 2.560838]\n",
      "1126 [D loss: 0.609091, acc: 68.75%] [G loss: 2.710687]\n",
      "1127 [D loss: 0.543285, acc: 76.56%] [G loss: 2.691441]\n",
      "1128 [D loss: 0.544469, acc: 76.56%] [G loss: 2.652805]\n",
      "1129 [D loss: 0.572167, acc: 68.75%] [G loss: 2.738454]\n",
      "1130 [D loss: 0.573441, acc: 67.19%] [G loss: 2.783965]\n",
      "1131 [D loss: 0.574297, acc: 71.88%] [G loss: 2.939991]\n",
      "1132 [D loss: 0.393214, acc: 84.38%] [G loss: 2.756447]\n",
      "1133 [D loss: 0.504166, acc: 73.44%] [G loss: 2.681370]\n",
      "1134 [D loss: 0.472056, acc: 85.94%] [G loss: 2.980350]\n",
      "1135 [D loss: 0.661014, acc: 67.19%] [G loss: 2.962057]\n",
      "1136 [D loss: 0.476743, acc: 82.81%] [G loss: 2.999122]\n"
     ]
    }
   ],
   "source": [
    "bigan.train(epochs=40000, batch_size=32, sample_interval=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
